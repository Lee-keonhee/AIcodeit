---
layout: post
title:  "위클리 페이퍼 #8"
date:   2025-08-17 09:00:00 +0900
categories: [딥러닝, 인공지능, 데이터과학]
tags: [딥러닝 기초]
---

[1. YOLO(You Only Look Once) 모델의 주요 특징과 장점은 무엇인가요?](#1-YOLO의 주요 특징 및 장점)

[2. mAP(mean Average Precision)의 개념이 무엇이며 객체 인식에서 어떻게 활용되는지 설명해주세요.](#2-mAP의 개념과 활용)


---
# 1. YOLO(You Only Look Once) 모델의 주요 특징과 장점은 무엇인가요?
---

## 1.1. YOLO(You Only Look Once) 모델의 주요 특징과 장점
YOLO는 객체 탐지 분야에 혁신을 가져온 One-stage(단일 단계) 모델입니다. 이전 모델들이 객체 탐지를 여러 단계로 나누어 처리한 것과 달리, YOLO는 이미지 전체를 한 번만 보고 바로 객체의 위치(바운딩 박스)와 클래스를 예측합니다. 이러한 방식 덕분에 YOLO는 압도적으로 빠른 추론 속도를 자랑하며, 실시간 객체 탐지 분야의 표준이 되었습니다.

---

## 1.2 YOLO의 버전별 특징
YOLO 모델은 크게 원저자인 조셉 레드몬(Joseph Redmon)이 개발한 초기 버전과, 이후 상용화와 대중화에 크게 기여한 Ultralytics가 개발한 버전으로 나눌 수 있습니다.

### 1.2.1 초기 개발 버전
#### YOLOv1

<img src="{{"/assets/images/yolov1.png" | relative_url }}" width="400" height="300" alt="yolov1">
<br>
**YOLOv1의 작동방식**
1. 입력 이미지를 S x S Grid로 나눔
<br>
2. 각 Grid Cell에 대해 Bounding Box와 Classification 두 가지 Task를 동시에 진행한다.
   - 2-1. 각 Grid cell은 B개의 Bounding Box를 계산한다.
     - 각 Bounding Box는 박스의 중심 x,y 좌표와 width, height, **Confidence Score**를 가진다.즉, YOLOv1의 출력 Bounding Box의 크기는 SxSx(Bx5)가된다.
     - Confidence Score는 박스가 물체를 포함하는 지에 대한 확률 값과 예측한 박스가 얼마나 정확한가에 대한 정도를 반영한다.
   - 2-2. Grid Cell은 동시에 Classification도 수행한다.
     - 여기서 주의할 점은 각 Bounding Box가 아닌 Grid Cell별로 분류를 수행한다.
     - 따라서 각 셀 SxS개 별로 Class의 갯수 C의 크기를 가져  SxSxC 의 형태의 output을 가지게 된다.
   - 따라서, 최종 Output의 형태는 S x S x(Bx5 + C)를 가지게 된다.
<br>
3. NMS(Non-Max Suppression)를 거쳐 겹치는 Box들을 제거하고, 최종 결과만 보여준다.
   - NMS(Non-Max Suppression)
     - 일반적으로 Object Detection 모델을 통과하게 되면, 아래 그림과 같이 하나의 Object에 여러 개의 Bounding box가 그려집니다.
     <img src="{{"/assets/images/nms.png" | relative_url }}" width="400" height="200" alt="yolov1">
     - 이렇게 <u>여러 Bounding Box 중에 가장 신뢰도 높은 Box만을 고르는 것</u>을 **NMS(Non-Max Suppression)** 이라고 합니다.
     - 수행과정
       1. Confidence score가 특정 Confidence threshold 값보다 낮은 Bounding box를 모두 제거
       2. 가장 높은 Confidence score를 가진 Bounding box 순으로 내림차순으로 정렬
       3. 맨 앞에 있는 Bounding box와 나머지 Bounding box와의 IoU를 계산하여 IoU가 특정 threshold 보다 큰 Bounding box는 제거
       4.  3의 과정을 모든 Bounding box에 순차적으로 적용

2016년 출시된 YOLOv1은 객체 탐지 분야에 혁신을 가져온 최초의 One-stage (단일 단계) 모델입니다. 이미지 전체를 단 한 번만 보고 바로 바운딩 박스와 객체 클래스를 예측하는 방식을 채택했습니다. 이 모델은 입력 이미지를 고정된 크기의 그리드(Grid)로 분할하고, 각 그리드 셀이 경계 상자(Bounding Box)와 해당 객체의 클래스 확률을 직접 예측하도록 설계되었습니다. 백본(Backbone) 네트워크로는 Darknet을 사용했습니다.

이러한 YOLOv1은 당시 다른 객체 탐지 모델들, 특히 Two-stage 방식의 모델들에 비해 압도적으로 빠른 속도라는 큰 장점을 가졌으며, 전체 탐지 과정이 단일 네트워크 내에서 이루어지는 End-to-End 학습이 가능했습니다.

- YOLOv1의 한계
Grid를 나눈 각 Cell마다 Bounding Box의 수가 B개로 정해져 있다는 것입니다. Bounding Box가 적으면 RoI가 적어지게 되고, 그에 따라 Object 탐지 능력이 떨어지게 됩니다. 이로 인해, 하나의 Cell에 여러 개의 object의 중심이 겹치면 하나의 Object만 탐지한다는 단점이 있습니다.
<br>

#### YOLOv2

<img src="{{"/assets/images/yolov2.png" | relative_url }}" width="400" height="300" alt="yolov2">
<br>

YOLOv1에서는 각 grid cell이 하나의 객체만 담당했기 때문에, 한 cell 안에 두 개 이상의 객체가 들어가면 하나만 탐지되고 나머지는 무시되는 문제가 있었습니다.

YOLOv2에서는 다음과 같은 개선으로 이 문제를 해결했습니다:
1. Anchor Box 도입
   - 각 grid cell이 **여러 개의 사전 정의된 박스(5개 정도)**를 가지도록 함
   - 서로 다른 크기와 비율의 객체를 동시에 탐지 가능
2. Objectness Score
   - 각 anchor box마다 객체 존재 확률을 계산
   - 여러 객체가 겹쳐도 각각의 anchor box가 독립적으로 탐지
3. Feature Map 기반 Grid
   - CNN feature map에서 grid를 나누어, 공간 정보를 더 정확하게 반영

결과적으로 YOLOv2는 **한 grid cell 안에 여러 객체가 있어도 탐지 가능**하고, 작은 객체나 다양한 비율의 객체 탐지 성능도 크게 향상되었습니다.

- YOLOv2의 한계
작은 객체 탐지 성능이 향상되었음에도 불구하고, 여전히 탐지가 어려운 문제점이 있었습니다. 뿐만 아니라, anchor box의 크기를 사전에 미리 정의해놓았기 때문에 해당 box의 크기에 학습이 영향을 받았습니다.
<br>

#### YOLOv3

YOLOv2에서는 작은 객체 탐지에 대한 한계를 완전히 극복하지 못했으며, anchor box의 크기에 의존적인 문제가 있었습니다.

YOLOv3의 구조
<img src="{{"/assets/images/yolov3.jpg" | relative_url }}" width="450" height="250" alt="yolov3">

YOLOv3에서는 다음과 같은 개선으로 이러한 문제를 해결했습니다:

1. **Multi-scale Detection(FPN 구조)**
   - 13×13, 26×26, 52×52 feature map을 사용하여 큰 객체와 작은 객체를 모두 탐지
   - 작은 객체는 높은 해상도의 feature map(52×52)에서 탐지하고, 큰 객체는 낮은 해상도 feature map(13×13)에서 탐지
   - Skip Connection을 통해 높은 해상도의 feature를 합성하여 작은 객체 탐지 성능 향상

2. **Darknet-53 백본**
   - Residual block을 도입하여 더 깊은 네트워크에서도 학습 안정성 유지
   - 더 강력한 특징 추출로 정확도 향상

3. **k-means Anchor Box**
   - 데이터셋에 맞춘 anchor box 크기와 비율 자동 설정
   - YOLOv2처럼 사전 정의된 anchor box에 의존하지 않음
   - 다양한 객체 크기에 유연하게 대응 가능

4. **Multi-label Classification**
   - Logistic classifier를 사용하여 클래스가 겹치는 객체도 탐지 가능
   - 단일 객체에 하나의 클래스만 제한되지 않음

- YOLOv3의 한계
 YOLOv2의 단점이었던 작은 객체 탐지 성능을 개선했지만, 여전히 속도가 느리다는 단점을 가지고 있었습니다. 특히 YOLOv3는 객체의 크기에 따라 3가지 다른 스케일(scale)의 그리드에서 예측을 수행했는데, 이로 인해 연산량이 증가했습니다.
<br>

### Ultralytics 개발 버전
#### YOLOv5
YOLOv4까지는 기존 Darknet이라는 자체 프레임워크를 사용하여 개발이 어렵다는 단점이 있었습니다.
YOLOv5는 YOLOv3의 핵심 아이디어를 계승하면서 개발 편의성과 성능을 대폭 개선한 모델입니다.
YOLOv5의 구조
<img src="{{"/assets/images/yolov5.jpg" | relative_url }}" width="450" height="250" alt="yolov5">

1. PyTorch 기반의 편리한 사용성: 
YOLOv3의 Darknet 프레임워크와 달리, PyTorch로 구현되어 개발자가 쉽게 모델을 수정하고 실험할 수 있습니다. 이는 YOLOv5의 가장 큰 장점 중 하나로, YOLO 시리즈의 대중화에 기여했습니다.
2. 효율적인 네트워크 구조: 
V5는 CSP(Cross Stage Partial) Darknet을 백본으로 사용합니다. 이 구조는 연산량을 줄여 모델의 추론 속도를 높이면서도 정확도를 유지합니다.
3. 다양한 모델 크기: 
YOLOv5n, s, m, l, x와 같이 다양한 모델 스케일을 제공합니다. 사용자는 장치 성능과 목표 정확도에 따라 최적의 모델을 선택할 수 있습니다.
4. 강화된 데이터 증강: 
Mosaic Augmentation 등 다양한 데이터 증강 기법을 적용하여 모델의 일반화 성능을 크게 향상시켰습니다.

- YOLOv5의 한계
  - Anchor Box 의존성: YOLOv5는 여전히 앵커 박스(anchor box) 기반의 객체 탐지 모델입니다. 앵커 박스의 크기를 사전에 정해두고 학습하기 때문에, 데이터셋에 따라 성능이 영향을 받을 수 있습니다.
  - 모델 경량화의 한계: YOLOv5는 다양한 크기의 모델을 제공하지만, 모바일 기기나 엣지 디바이스에서 사용하기에는 여전히 무거운 편입니다.

#### YOLOv8
YOLOv8은 YOLOv5의 뒤를 잇는 모델로, 사용 편의성과 성능을 동시에 높였습니다.
YOLOv8의 구조
<img src="{{"/assets/images/yolov8.jpg" | relative_url }}" width="300" height="500" alt="yolov8">


1. Anchor-Free Detection: 
앵커 박스 개념을 사용하지 않아 모델 헤드가 단순화되고 학습이 유연해졌습니다.
2. C2f 모듈: 
새로운 백본 및 Neck 구조를 도입하여 효율적인 특징 추출을 가능하게 했습니다.
3. 통합 기능: 
객체 탐지뿐만 아니라 **분류(Classification)**와 **세그멘테이션(Segmentation)**을 한 프레임워크에서 지원하여 활용 범위를 넓혔습니다.
4. 손실 함수: 
**VFL(Varifocal Loss)**과 **DFL(Distribution Focal Loss)**을 사용하여 예측 정확도를 개선했습니다.

- YOLOv8의 한계
앵커 기반 모델에 익숙한 사용자에게는 새로운 Anchor-Free 방식에 대한 적응이 필요합니다. 또한, 이전 모델들에 비해 상대적으로 더 복잡한 구조를 가지고 있어 연산량이 많습니다.

#### YOLOv11
YOLOv11은 작은 객체 탐지 성능을 획기적으로 향상시키는 데 중점을 둔 모델입니다.
1. 듀얼 헤드(Dual-Head) 아키텍처: 
작은 객체와 큰 객체를 탐지하는 두 개의 헤드를 사용하여, 특히 작은 객체에 대한 탐지 정확도를 높였습니다.
2. 효율성: 
새로운 손실 함수와 학습 전략을 통해 더 적은 파라미터로도 높은 정확도를 달성합니다.
3. 다목적성: 
효율성과 정확도를 모두 갖춘 모델로, 다양한 환경에 적용할 수 있습니다.

- YOLOv8의 한계
듀얼 헤드 구조로 인해 모델의 복잡성이 증가할 수 있으며, 이로 인한 추론 속도 저하 가능성이 있습니다.

#### YOLOv12
2025년 2월에 출시된 YOLOv12는 주의 중심(Attention-Centric) 아키텍처를 도입하여 YOLO 시리즈의 새로운 방향을 제시했습니다. 영역 어텐션(Area Attention) 메커니즘을 통해 정확도를 높이면서도 실시간 추론 속도를 유지하며, 이전 모델들보다 더 효율적인 성능을 보여줍니다.


---
## 2. mAP(mean Average Precision)의 개념이 무엇이며 객체 인식에서 어떻게 활용되는지 설명해주세요.

---

**mAP (mean Average Precision)** 는 객체 탐지 모델의 성능을 종합적으로 평가하는 지표입니다. 모델이 이미지 내의 여러 객체들을 얼마나 정확하게 **탐지(Bounding Box의 위치)** 하고 **분류(객체 클래스)** 하는지 측정합니다.

mAP를 이해하기 위해서는 몇 가지 핵심 개념을 알아야 합니다.

### 2.1 mAP의 구성 요소
1. IoU (Intersection over Union):
   - 모델이 예측한 바운딩 박스(Predicted BBox)와 실제 정답 바운딩 박스(Ground Truth BBox)가 얼마나 겹치는지를 측정하는 비율입니다.
   - IoU = (두 박스의 겹치는 영역) / (두 박스를 합친 전체 영역)  
   - IoU 값이 높을수록 모델이 객체의 위치를 더 정확하게 예측했다는 의미입니다. 일반적으로 모델의 예측이 'True Positive (정답)'로 인정받기 위한 최소 IoU 임계값을 설정합니다 (예: 0.5, 0.75).
   <br>
2. Precision (정밀도) & Recall (재현율):
   - Precision: 모델이 '긍정이라고 예측한 것들 중에서 실제로 긍정인 비율' 입니다. ( TP / (TP + FP) )
     - 객체 탐지에서는 "모델이 탐지한 바운딩 박스들 중 실제 객체를 정확히 찾아낸 비율"을 의미합니다. (오탐지(False Positive)가 적을수록 높습니다.)
   - Recall: 실제 '긍정인 것들 중에서 모델이 긍정이라고 예측한 비율' 입니다. ( TP / (TP + FN) )
     - 객체 탐지에서는 "실제 이미지에 있는 모든 객체들 중 모델이 찾아낸 비율"을 의미합니다. (놓친 객체(False Negative)가 적을수록 높습니다.)
   - 객체 탐지 모델은 각 예측에 대해 '신뢰도 점수(Confidence Score)'를 출력하는데, 이 신뢰도 점수의 임계값을 조절함에 따라 Precision과 Recall이 변화합니다. 신뢰도 임계값을 높이면 Precision은 올라가지만 Recall은 낮아지고, 그 반대도 마찬가지입니다.
   <br>
3. Precision-Recall (P-R) Curve:
   - 모델의 신뢰도 점수 임계값을 0부터 1까지 변화시키면서 얻어지는 모든 Precision과 Recall 쌍을 그래프로 그린 것입니다.
   - 일반적으로 Recall을 x축, Precision을 y축으로 그립니다.
   <br>
4. AP (Average Precision):
   - 특정 하나의 객체 클래스에 대한 P-R 곡선 아래의 면적을 의미합니다.  
   - 이 면적이 넓을수록 해당 클래스에 대한 모델의 성능이 좋다고 평가합니다. 즉, 모든 Recall 수준에서 높은 Precision을 유지한다는 의미입니다.
   <br>
5. mAP (mean Average Precision):
   - 전체 데이터셋에 있는 모든 객체 클래스(예: dog, cat, car 등)에 대한 AP 값을 각각 계산한 뒤, 그 값들을 평균 낸 것입니다.  
   - 이는 모델이 다양한 종류의 객체를 전반적으로 얼마나 잘 탐지하는지 나타내는 포괄적인 성능 지표입니다.

### 2.2 IoU 임계값의 중요성 (mAP@50 vs mAP@50-95)
mAP는 IoU 임계값을 어떻게 설정하느냐에 따라 값이 달라집니다.

- mAP@0.5 (또는 mAP@50):
  - IoU 임계값을 0.5로 설정하여 계산한 mAP입니다.  
  - 예측 박스가 정답 박스와 절반 이상만 겹쳐도 정답으로 인정하는 비교적 관대한 기준입니다. PASCAL VOC 챌린지에서 주로 사용되었습니다.
- mAP@0.5:0.95 (또는 mAP@50-95):
  - IoU 임계값을 0.5부터 0.95까지 0.05 단위로 (0.5, 0.55, ..., 0.95) 총 10개의 임계값에 대해 mAP를 각각 계산한 후, 이 10개의 mAP 값들을 모두 평균 낸 것입니다.
  - 예측 박스가 정답 박스와 아주 정교하게 겹쳐야 정답으로 인정하는 엄격하고 포괄적인 기준입니다. COCO 챌린지에서 사용되는 표준 지표이며, 실제 환경에서의 모델 정밀도를 더 잘 반영합니다.

### 2.3 객체 탐지에서의 활용
  mAP는 객체 탐지 모델에서 다음과 같이 활용됩니다.

1. 성능 평가의 표준 지표: 개발된 객체 탐지 모델이 얼마나 잘 작동하는지 측정하는 가장 중요한 척도입니다. 모델의 전반적인 정확도와 위치 추정 능력을 동시에 평가합니다.
2. 모델 비교 및 벤치마킹: 다양한 객체 탐지 모델(예: YOLO, SSD, Faster R-CNN)의 성능을 객관적으로 비교하고, 특정 데이터셋에서 어떤 모델이 가장 우수한지 판단하는 기준이 됩니다. 새로운 모델이나 개선된 알고리즘을 발표할 때 이 mAP를 핵심 결과로 제시합니다.
3. 하이퍼파라미터 튜닝 및 모델 개선 가이드: 모델 학습 과정에서 손실 함수(Loss Function), 학습률(Learning Rate), 아키텍처 변경 등 다양한 하이퍼파라미터나 구조적 변화가 모델 성능에 어떤 영향을 미치는지 mAP를 통해 파악하고, 모델을 최적화하는 데 활용됩니다.
4. 연구 및 개발의 핵심 목표: 딥러닝 연구에서 새로운 객체 탐지 기술을 개발할 때, 목표 성능을 mAP 값으로 설정하고 이를 달성하기 위해 연구를 진행합니다.

결론적으로 mAP는 객체 탐지 모델의 종합적인 성능을 측정하고, 연구 및 산업 분야에서 모델을 개발하고 비교하는 데 있어 핵심적인 역할을 하는 매우 중요한 지표입니다.