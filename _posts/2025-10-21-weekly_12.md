---
layout: post
title:  "위클리 페이퍼 #12"
date:   2025-10-21 15:00:00 +0900
categories: [딥러닝, 인공지능, 데이터과학]
tags: [딥러닝 기초]
---

[1. LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요? 여러 LLM 서비스들은 할루시네이션 문제를 어떻게 극복하려고 시도 중일까요?](#subject1)

[2. 모델 크기를 키우는 것만으로는 성능이 일정 시점 이후 둔화되는 이유는 무엇일까요?](#subject2)

[3. PEFT가 필요한 이유는 무엇이며, 어떤 상황에서 특히 효과적인가요?](#subject3)

---
# 1. LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요? 여러 LLM 서비스들은 할루시네이션 문제를 어떻게 극복하려고 시도 중일까요? {#subject1}
---

ChatGPT에게 "세종대왕이 한글을 발명한 연도는?"이라고 물으면 정확히 "1443년"이라고 답합니다. 하지만 "세종대왕이 컴퓨터를 발명한 연도는?"이라고 물으면 어떻게 될까요? 놀랍게도 모델이 그럴듯한 거짓 정보를 생성할 수 있습니다. 이것이 바로 **할루시네이션(Hallucination)** 문제입니다.

## 1.1 할루시네이션이란?

### 1.1.1 정의와 예시

**할루시네이션**은 LLM이 사실과 다르거나 존재하지 않는 정보를 마치 사실인 것처럼 그럴듯하게 생성하는 현상입니다. 마치 사람이 환각을 보듯이, AI가 "없는 것을 있다고" 말하는 것이죠.

**실제 사례들:**

1. **존재하지 않는 논문 인용**
   - 질문: "기후변화에 관한 최근 연구를 추천해주세요"
   - LLM: "Smith et al. (2023)의 'Global Warming Trends'를 추천합니다. Nature 지에 게재된..."
   - 문제: 이런 논문은 실제로 존재하지 않음

2. **잘못된 역사적 사실**
   - 질문: "나폴레옹이 워털루 전투에서 승리했나요?"
   - LLM: "네, 나폴레옹은 1815년 워털루 전투에서 승리하여..."
   - 사실: 나폴레옹은 워털루에서 패배함

3. **허구의 통계 데이터**
   - 질문: "한국의 2024년 실업률은?"
   - LLM: "2024년 한국의 실업률은 2.3%입니다"
   - 문제: 구체적인 수치를 지어냄

### 1.1.2 할루시네이션의 유형

**1) 사실적 할루시네이션 (Factual Hallucination)**
- 객관적으로 검증 가능한 사실을 틀리게 진술
- 예: 날짜, 이름, 숫자 등의 오류

**2) 충실도 할루시네이션 (Faithfulness Hallucination)**
- 주어진 입력 문맥과 모순되는 내용 생성
- 예: 문서 요약 시 원문에 없는 내용 추가

**3) 지식 갭 할루시네이션**
- 모델의 학습 데이터에 없는 정보에 대해 추측
- 예: 최신 뉴스, 특정 전문 분야 지식

## 1.2 왜 할루시네이션이 문제가 되는가?

### 1.2.1 신뢰성 문제

LLM은 틀린 정보도 **매우 확신에 찬 어조**로 전달합니다. 이는 다음과 같은 위험을 초래합니다:

**의료 분야:**
- 잘못된 의학 정보로 건강 피해 발생 가능
- 예: "이 증상에는 A 약물이 효과적입니다" (실제로는 금기 사항)

**법률 분야:**
- 2023년 미국에서 실제 사건: 변호사가 ChatGPT가 생성한 가짜 판례를 법정에 제출
- 존재하지 않는 법률 조항 인용으로 소송 패소

**학술 연구:**
- 존재하지 않는 참고문헌 인용
- 연구의 신뢰성과 재현성 훼손

### 1.2.2 확산성 문제

할루시네이션으로 생성된 잘못된 정보가:
- 인터넷에 퍼져 다른 사람들이 재인용
- 결국 새로운 LLM의 학습 데이터로 포함
- 오류가 증폭되는 악순환

<div style="background-color: #3F2F2F; padding: 15px; border-left: 4px solid #ff4444; margin: 20px 0;">
<strong>⚠️ 주의:</strong> LLM의 유창한 문장과 자신감 있는 어조에 속지 마세요. 중요한 정보는 항상 다른 신뢰할 수 있는 출처로 검증이 필요합니다.
</div>

## 1.3 왜 할루시네이션이 발생하는가?

### 1.3.1 LLM의 작동 원리에 내재된 한계

**1) 확률적 생성 방식**
- LLM은 "사실"을 암기하는 것이 아니라 **패턴을 학습**
- "다음 단어로 뭐가 올 확률이 높은가?"를 계산
- 사실 여부보다는 "그럴듯함"을 우선

**2) 학습 데이터의 한계**
- 인터넷의 모든 정보가 정확하지 않음
- 상충되는 정보들이 혼재
- 최신 정보는 학습되지 않음

**3) 압축 손실**
- 수 TB의 데이터를 수백 GB 모델로 압축
- 모든 지식을 완벽히 저장할 수 없음
- 압축 과정에서 정보 왜곡 발생

### 1.3.2 구체적인 발생 메커니즘

**시나리오 1: 지식 경계에서의 추측**
```
질문: "2025년 노벨 물리학상 수상자는?"
모델의 내부 상태:
- 학습 데이터는 2025년 1월까지
- 노벨상은 10월에 발표
- 알 수 없지만, 그럴듯한 답변을 생성해야 함
→ 과거 수상자 패턴을 바탕으로 허구의 이름 생성
```

**시나리오 2: 모호한 질문에서의 오버피팅**
```
질문: "그 영화 어땠어?"
모델의 내부 상태:
- "그 영화"가 무엇인지 불명확
- 하지만 대화를 이어가야 함
→ 일반적인 영화 리뷰 패턴으로 구체적인 허구 생성
```

## 1.4 할루시네이션 극복을 위한 산업계의 시도

### 1.4.1 검색 증강 생성 (RAG: Retrieval-Augmented Generation)

**핵심 아이디어:** 답변 전에 관련 문서를 먼저 검색하고, 그 정보를 기반으로 답변 생성

**작동 방식:**
```
1. 사용자 질문: "2024년 한국 GDP 성장률은?"
2. 실시간 검색: 신뢰할 수 있는 데이터베이스/인터넷 검색
3. 검색 결과 획득: "한국은행 발표 3.2% 성장"
4. 검색 결과 기반 답변: "한국은행에 따르면 2024년 GDP 성장률은 3.2%입니다."
```

**주요 서비스들의 구현:**

**1) Perplexity AI**
- 모든 답변에 출처 링크 제공
- 실시간 웹 검색 결과와 LLM 결합
- 각 문장마다 [1], [2] 같은 인용 번호 표시

**2) Microsoft Copilot (Bing Chat)**
- Bing 검색 엔진과 GPT-4 통합
- 답변 하단에 참조 링크 나열
- "웹 검색 결과를 바탕으로..." 명시

**3) ChatGPT (웹 브라우징 모드)**
- 플러그인 또는 내장 브라우징 기능
- 최신 정보 검색 후 답변
- 검색한 페이지 URL 공개

**장점:**
- 최신 정보 접근 가능
- 출처 추적 가능
- 환각 크게 감소

**한계:**
- 검색 결과 자체가 부정확할 수 있음
- 검색 품질에 성능 의존
- 응답 속도 느려짐

### 1.4.2 인용 및 출처 표시

**Google Gemini의 접근:**
- 답변에 "접지(Grounding)" 개념 도입
- 각 주장마다 Google 검색 결과 링크
- 사용자가 직접 확인 가능

**Claude (Anthropic)의 접근:**
- "Constitutional AI" 방식
- 불확실한 경우 솔직히 인정
- "제 학습 데이터에는 이 정보가 없어 확실하지 않습니다"

### 1.4.3 불확실성 표현 학습

**Calibrated Confidence (보정된 신뢰도)**

모델이 답변의 확실성을 정직하게 표현하도록 학습:

```
높은 확신: "세종대왕은 한글을 창제했습니다."
중간 확신: "이 정보는 일부 출처에서 확인되지만, 완전히 검증되지 않았을 수 있습니다."
낮은 확신: "죄송하지만, 이 질문에 대한 확실한 답변을 드리기 어렵습니다."
```

**OpenAI의 시도:**
- GPT-4에서 불확실성 토큰 추가
- "아마도", "~일 수 있습니다", "확실하지 않지만" 같은 표현 증가

### 1.4.4 사실 검증 레이어

**다단계 검증 시스템:**

```
1단계: LLM이 초안 생성
2단계: 팩트체크 모델이 각 주장 검증
3단계: 검증 실패 시 해당 부분 수정 또는 제거
4단계: 최종 답변 출력
```

**Google의 Search Grounding:**
- 생성된 답변을 Google 검색 결과와 대조
- 일치하지 않는 내용 자동 수정
- 검증 불가능한 주장 삭제

### 1.4.5 강화 학습을 통한 정직성 향상

**RLHF (Reinforcement Learning from Human Feedback) 개선:**

전통적 RLHF의 문제:
- "유창하고 자신감 있는" 답변에 높은 보상
- 사실 여부보다 "그럴듯함"에 최적화

개선된 접근:
- **정확성에 더 높은 가중치**
- 틀린 답변에 큰 패널티
- "모르겠습니다"에 긍정적 보상 (적절한 경우)

**Anthropic의 Constitutional AI:**
- 명시적인 "정직성" 원칙 주입
- "사실이 불확실하면 추측하지 말 것"
- 자가 비판 및 수정 메커니즘

### 1.4.6 도메인 특화 미세조정

**전문 분야별 정확도 향상:**

**의료 분야: Med-PaLM (Google)**
- 의학 교과서와 논문으로 추가 학습
- 의사들의 피드백으로 미세조정
- 의학 면허 시험 수준 정확도

**법률 분야: LegalBERT, LexGPT**
- 판례와 법령 데이터 특화 학습
- 정확한 조문 인용 강화
- 법률 자문 전문가 검수

**과학 분야: Galactica (Meta)**
- 과학 논문 4800만 편 학습
- 수식, 화학식 등 정확한 표현
- 논문 인용 시스템 내장

<div style="background-color: #2F3F2F; padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;">
<strong>✅ 최선의 실천 방법:</strong><br>
완벽한 해결책은 없습니다. 여러 기법을 조합하는 것이 현재로서는 최선입니다:
<br><br>
• RAG + 출처 표시<br>
• 불확실성 표현 + 사실 검증<br>
• 도메인 특화 + 인간 검수
</div>

## 1.5 사용자가 할 수 있는 대응

### 1.5.1 효과적인 프롬프팅

**나쁜 예:**
```
"인공지능의 역사를 설명해줘"
→ 너무 광범위하여 할루시네이션 가능성 높음
```

**좋은 예:**
```
"1950년대 앨런 튜링의 기여를 중심으로, 인공지능의 초기 역사를 설명해줘. 
불확실한 부분은 명시해줘."
→ 구체적 범위 + 출처 요청
```

### 1.5.2 교차 검증

**핵심 원칙:** 중요한 정보는 항상 다른 출처로 확인

1. **LLM 답변 얻기**
2. **출처 요청**: "이 정보의 출처를 알려줘"
3. **직접 확인**: 언급된 출처를 실제로 찾아보기
4. **다른 LLM과 비교**: 여러 서비스에서 같은 질문
5. **전문가 확인**: 중요한 결정은 전문가 자문

### 1.5.3 비판적 사고

**경고 신호들:**
- 너무 구체적인 날짜나 숫자 (출처 없이)
- 모호한 표현: "연구에 따르면", "전문가들은 말한다"
- 최신 사건에 대한 확신 있는 답변
- 논란의 여지가 있는 주제에 대한 단정적 진술

## 1.6 미래 전망

### 1.6.1 기술적 발전 방향

**단기 (2025-2026):**
- RAG 시스템 고도화
- 실시간 팩트체크 통합
- 출처 투명성 강화

**중기 (2027-2028):**
- 자가 검증 능력 향상
- 외부 지식 베이스와의 긴밀한 통합
- 전문 도메인별 특화 모델 확산

**장기 (2029+):**
- 인과 관계 이해 능력 향상
- 지식 그래프 기반 추론
- 완전한 투명성과 설명 가능성

### 1.6.2 규제와 표준

**EU AI Act:**
- 고위험 AI 시스템 규제
- 할루시네이션 비율 공개 의무화 논의

**미국 NIST 프레임워크:**
- AI 신뢰성 평가 기준
- 할루시네이션 측정 지표 개발

<div style="background-color: #3F2F2F; padding: 20px; border-left: 4px solid #FFD700; margin: 20px 0;">
<strong>💡 핵심 정리:</strong><br><br>

<strong>할루시네이션이란?</strong><br>
LLM이 사실과 다른 정보를 그럴듯하게 생성하는 현상<br><br>

<strong>왜 문제인가?</strong><br>
• 신뢰성 문제 (의료, 법률 등 중요 분야)<br>
• 잘못된 정보의 확산<br>
• AI에 대한 신뢰 저하<br><br>

<strong>극복 방법:</strong><br>
• RAG (검색 증강 생성)<br>
• 출처 표시 및 인용<br>
• 불확실성 정직한 표현<br>
• 사실 검증 시스템<br>
• 도메인 특화 학습<br><br>

<strong>사용자의 역할:</strong><br>
중요한 정보는 반드시 교차 검증하고, LLM을 보조 도구로만 활용하세요!
</div>

---
# 2. 모델 크기를 키우는 것만으로는 성능이 일정 시점 이후 둔화되는 이유는 무엇일까요? {#subject2}
---

"모델을 크게 만들면 성능이 좋아진다"는 명제는 2010년대 후반 딥러닝의 황금률이었습니다. GPT-2(15억 개 파라미터)에서 GPT-3(1750억 개)로 넘어가며 놀라운 성능 향상을 보였죠. 하지만 무한정 크기만 키운다고 해서 계속 좋아질까요? 현실은 그렇게 단순하지 않습니다.

## 2.1 Scaling Laws: 크기와 성능의 관계

### 2.1.1 초기 발견: "Bigger is Better"

**OpenAI의 2020년 연구:**

2020년 OpenAI는 "Scaling Laws for Neural Language Models"라는 논문에서 놀라운 발견을 발표했습니다:

```
Loss ∝ N^(-α)

Loss: 모델 손실(낮을수록 좋음)
N: 파라미터 수
α: 스케일링 지수 (약 0.076)
```

**핵심 발견:**
- 파라미터가 10배 증가 → 손실이 일정 비율로 감소
- 이 관계가 여러 크기 범위에서 놀랍도록 일관됨
- "데이터, 컴퓨팅, 모델 크기를 동시에 키우면 계속 좋아진다"

**당시의 낙관론:**
- GPT-4, GPT-5는 단순히 더 크게 만들면 된다?
- 1조, 10조 파라미터 모델이 AGI로 가는 길?

### 2.1.2 현실의 벽

하지만 2023-2024년을 거치며 업계는 중요한 사실을 깨닫습니다:

**비용 폭발:**
- GPT-4 학습 비용: 약 1억 달러 추정
- 10배 더 큰 모델: 10억 달러?
- 100배 더 큰 모델: 100억 달러???

**실질적 한계 도달:**
- OpenAI CEO 샘 알트만: "GPT-4 이후 단순 스케일링 시대는 끝났다"
- Google DeepMind: "효율성이 새로운 키워드"
- Meta: "작지만 강한 모델"에 집중

## 2.2 수확 체감의 법칙이 작동하는 이유

### 2.2.1 이론적 한계

**1) 데이터 품질의 한계**

인터넷의 고품질 텍스트는 **유한**합니다:

```
추정치 (2024년 기준):
- 전체 인터넷 텍스트: ~50조 토큰
- 고품질 필터링 후: ~10조 토큰
- 중복 제거 후: ~5조 토큰
```

**문제:**
- GPT-3은 이미 ~3000억 토큰으로 학습
- GPT-4는 ~13조 토큰 추정
- 더 큰 모델은? 데이터가 부족!

**데이터 부족의 결과:**
- 같은 데이터 반복 → 과적합
- 저품질 데이터 추가 → 성능 저하
- 합성 데이터 생성 → 편향 증폭

**2) 최적화의 어려움**

모델이 커질수록 학습이 기하급수적으로 어려워집니다:

**Gradient 소실/폭발:**
- 1000층 모델의 backpropagation
- 앞쪽 레이어에 학습 신호 전달 어려움
- 특수한 정규화 기법 필요

**수치 불안정성:**
- 부동소수점 연산 오차 누적
- Mixed precision 학습의 한계
- 특정 구간에서 학습 붕괴

**3) 표현력의 포화**

**Universal Approximation Theorem의 함정:**

이론적으로는 충분히 큰 신경망이 모든 함수를 근사할 수 있습니다. 하지만:

- "충분히 크다"가 실용적 범위를 넘어설 수 있음
- 특정 복잡도 이상은 exponential하게 많은 파라미터 필요
- 실제로는 구조적 개선이 더 효과적

### 2.2.2 실용적 한계

**1) 메모리 병목**

**GPU 메모리 한계:**
```
NVIDIA H100 GPU:
- 메모리: 80GB
- 1750억 파라미터 GPT-3 로드: 약 350GB 필요 (float16)
- 해결: 4개 이상의 GPU로 분산
```

**더 큰 모델:**
```
1조 파라미터 모델:
- 메모리: 2TB 이상
- 필요 GPU: 수십~수백 개
- 통신 오버헤드 급증
```

**2) 추론 비용**

사용자가 체감하는 현실적 문제:

| 모델 크기 | 추론 시간 | 비용/1M 토큰 | 적용 가능성 |
|-----------|-----------|--------------|-------------|
| 7B | 0.1초 | $0.1 | 실시간 앱 |
| 70B | 1초 | $1 | 챗봇 |
| 175B | 3초 | $5 | 전문 작업 |
| 1T+ | 10초+ | $20+ | 연구용 |

**실제 서비스 관점:**
- ChatGPT 하루 사용자: 수억 건 쿼리
- 응답 시간 1초 증가 → 사용자 이탈 증가
- 비용 2배 → 수익성 악화

**3) 에너지 소비**

**GPT-3 학습:**
- 전력 소비: 약 1,287 MWh
- CO2 배출: 약 552톤
- 비용: 약 460만 달러

**10배 큰 모델:**
- 단순 계산으로도 10배 이상
- 냉각, 인프라 비용 추가
- 환경적 지속가능성 문제

## 2.3 수확 체감을 보여주는 실증 사례

### 2.3.1 벤치마크 성능 정체

**MMLU (대학 수준 지식 평가) 점수:**

```
2020: GPT-3 (175B) - 43.9%
2023: GPT-4 (추정 1.7T) - 86.4%
2024: GPT-4o (개선 버전) - 87.2%
2025: Gemini 2.5 Pro - 88.7%
```

**관찰:**
- GPT-3 → GPT-4: 크기 10배, 성능 2배
- GPT-4 → GPT-4o: 크기 비슷, 성능 1% 미만 향상
- 90%를 넘기기가 점점 어려워짐

### 2.3.2 Chinchilla의 발견 (DeepMind, 2022)

**충격적인 결론:** "우리는 모델을 잘못 키워왔다!"

**기존 관행:**
- 큰 모델 + 적당한 데이터 양

**Chinchilla 연구:**
- 70B 파라미터 Chinchilla vs 280B Gopher
- **같은 컴퓨팅 비용**
- 결과: Chinchilla가 더 나은 성능!

**핵심 교훈:**
```
최적 비율 = 
파라미터 수 : 학습 토큰 수 = 1 : 20

즉, 1B 파라미터 → 20B 토큰으로 학습
```

**실무적 함의:**
- GPT-3(175B)는 "과소학습" 상태였음
- 같은 비용으로 더 작지만 더 많은 데이터로 학습하는 것이 효율적
- LLaMA, Mistral 같은 모델들이 이 원칙 따름

### 2.3.3 LLaMA의 성공

**Meta의 LLaMA 시리즈가 증명:**

**LLaMA 2 (2023):**
- 70B 파라미터 (GPT-3의 40%)
- 2조 토큰 학습 (GPT-3의 6배)
- 많은 벤치