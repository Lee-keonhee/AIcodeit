---
layout: post
title: "LangChain RAG ì‹œìŠ¤í…œ ì™„ë²½ ê°€ì´ë“œ 2025"
date: 2025-11-09
categories: [ë”¥ëŸ¬ë‹, ì¸ê³µì§€ëŠ¥, LLM]
tags: [RAG, LangChain, Agent, í‰ê°€]
---

# LangChainì„ í™œìš©í•œ RAG ì‹œìŠ¤í…œ ì™„ë²½ ê°€ì´ë“œ

## ëª©ì°¨
1. [RAG ì‹œìŠ¤í…œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ](#section1)
2. [RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë°©ë²•](#section2)
3. [RAG Agent: ê°œë…ê³¼ êµ¬í˜„](#section3)
4. [ì‹¤ì „ êµ¬í˜„ ì˜ˆì œ](#section4)
5. [ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ì™€ ìµœì‹  íŠ¸ë Œë“œ](#section5)

---

# 1. RAG ì‹œìŠ¤í…œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ {#section1}

## 1.1 RAGë€ ë¬´ì—‡ì¸ê°€?

**RAG (Retrieval-Augmented Generation)**ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ì„ ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ê³¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ì™¸ë¶€ ê²€ìƒ‰ê¸°(retriever)ë¥¼ í†µí•´ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

### 1.1.1 RAGì˜ í•µì‹¬ ì›ë¦¬

```
ì‚¬ìš©ì ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ê´€ë ¨ ë¬¸ì„œ íšë“ â†’ LLMì— ì»¨í…ìŠ¤íŠ¸ ì œê³µ â†’ ë‹µë³€ ìƒì„±
```

**RAGê°€ í•´ê²°í•˜ëŠ” ë¬¸ì œ:**
- âœ… **í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ**: ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•œ ë‹µë³€
- âœ… **ìµœì‹  ì •ë³´ ì œê³µ**: ì¬í•™ìŠµ ì—†ì´ ì§€ì‹ ì—…ë°ì´íŠ¸
- âœ… **ì¶œì²˜ ì¶”ì **: íˆ¬ëª…í•œ ì •ë³´ ì œê³µ
- âœ… **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ë¶„ì•¼ ë¬¸ì„œ í™œìš©

## 1.2 LangChain RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### 1.2.1 ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ (Data Ingestion & Preprocessing)

**ì—­í• **: ë‹¤ì–‘í•œ í˜•íƒœì˜ ì›ì‹œ ë°ì´í„°ë¥¼ ì‹œìŠ¤í…œì— ë¡œë“œí•˜ê³  í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

**ì£¼ìš” ì»´í¬ë„ŒíŠ¸:**

**1) Document Loaders**
```python
from langchain.document_loaders import (
    TextLoader,           # í…ìŠ¤íŠ¸ íŒŒì¼
    PDFLoader,           # PDF ë¬¸ì„œ
    WebBaseLoader,       # ì›¹í˜ì´ì§€
    DirectoryLoader,     # ë””ë ‰í† ë¦¬ ì „ì²´
    CSVLoader,           # CSV íŒŒì¼
    UnstructuredLoader   # ë¹„ì •í˜• ë¬¸ì„œ
)

# ì˜ˆì‹œ: ì›¹ í˜ì´ì§€ ë¡œë“œ
loader = WebBaseLoader("https://blog.example.com/post")
documents = loader.load()
```

**ì§€ì›í•˜ëŠ” 160+ ë°ì´í„° ì†ŒìŠ¤:**
- ë¬¸ì„œ: PDF, Word, Markdown, HTML
- ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL, MongoDB, MySQL
- API: Notion, Google Drive, Confluence
- í´ë¼ìš°ë“œ: S3, Azure Blob, GCS

**2) Text Splitters (ì²­í¬ ë¶„í• )**

ì „ì²´ ë¬¸ì„œë¥¼ ê´€ë¦¬ ê°€ëŠ¥í•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

```python
from langchain.text_splitters import RecursiveCharacterTextSplitter

# ì˜ë¯¸ë¡ ì  ì²­í¬ ë¶„í• 
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # ì²­í¬ í¬ê¸°
    chunk_overlap=200,      # ì¤‘ë³µ ì˜ì—­ (ë§¥ë½ ìœ ì§€)
    length_function=len,
    separators=["\n\n", "\n", " ", ""]  # ë¶„í•  ìš°ì„ ìˆœìœ„
)

chunks = splitter.split_documents(documents)
```

**ì²­í¬ ë¶„í•  ì „ëµ:**

| ì „ëµ | ì„¤ëª… | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|------|------|--------------|
| **Fixed-size** | ê³ ì • í¬ê¸° ë¶„í•  | ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ |
| **Semantic** | ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  | ê¸°ìˆ  ë¬¸ì„œ, ë…¼ë¬¸ |
| **Contextual** | í—¤ë” í¬í•¨ ë¶„í•  | êµ¬ì¡°í™”ëœ ë¬¸ì„œ |
| **Recursive** | ì¬ê·€ì  ë¶„í•  | ëŒ€ë¶€ë¶„ì˜ ê²½ìš° (ê¶Œì¥) |

**ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤:**
```python
# 2025ë…„ ê¶Œì¥ ì„¤ì •
chunk_size = 512~1000 í† í°     # ë„ˆë¬´ ì‘ìœ¼ë©´ ë§¥ë½ ì†ì‹¤, ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆ
chunk_overlap = 100~200 í† í°   # ë¬¸ë§¥ ì—°ì†ì„± ìœ ì§€
```

### 1.2.2 ì„ë² ë”© (Embeddings)

**ì—­í• **: í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°

**ì£¼ìš” ì„ë² ë”© ëª¨ë¸ (2025ë…„ ê¸°ì¤€):**

```python
from langchain.embeddings import (
    OpenAIEmbeddings,           # OpenAI text-embedding-3
    HuggingFaceEmbeddings,      # SBERT, all-MiniLM
    CohereEmbeddings,           # Cohere Embed v3
    GoogleVertexAIEmbeddings    # Gecko Embedding
)

# ë¡œì»¬ ì„ë² ë”© (ë¬´ë£Œ, í”„ë¼ì´ë²„ì‹œ)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'}
)

# ìƒìš© ì„ë² ë”© (ê³ ì„±ëŠ¥)
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",
    dimensions=3072  # ì¶•ì†Œ ê°€ëŠ¥: 256, 512, 1024
)
```

**ì„ë² ë”© ëª¨ë¸ ë¹„êµ:**

| ëª¨ë¸ | ì°¨ì› | ì„±ëŠ¥ | ë¹„ìš© | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|------|------|----------|
| **OpenAI text-embedding-3-large** | 3072 | â­â­â­â­â­ | ğŸ’°ğŸ’° | í”„ë¡œë•ì…˜ |
| **Cohere Embed v3** | 1024 | â­â­â­â­â­ | ğŸ’°ğŸ’° | ë‹¤êµ­ì–´ |
| **all-MiniLM-L6-v2** | 384 | â­â­â­ | ë¬´ë£Œ | í”„ë¡œí† íƒ€ì… |
| **BGE-large** | 1024 | â­â­â­â­ | ë¬´ë£Œ | ë¡œì»¬ ë°°í¬ |

### 1.2.3 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (Vector Store)

**ì—­í• **: ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰

**ì£¼ìš” ë²¡í„° DB (2025ë…„):**

```python
from langchain.vectorstores import (
    FAISS,           # ë¡œì»¬, ë¹ ë¦„
    Chroma,          # ë¡œì»¬, ê°„í¸
    Pinecone,        # í´ë¼ìš°ë“œ, í™•ì¥ì„±
    Weaviate,        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    Qdrant,          # ì˜¤í”ˆì†ŒìŠ¤, í”„ë¡œë•ì…˜
    Milvus           # ëŒ€ê·œëª¨, ë¶„ì‚°
)

# FAISS ì˜ˆì‹œ (ë¡œì»¬ ê°œë°œ)
from langchain.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)

# Pinecone ì˜ˆì‹œ (í”„ë¡œë•ì…˜)
import pinecone
from langchain.vectorstores import Pinecone

vectorstore = Pinecone.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name="rag-index"
)
```

**ë²¡í„° DB ì„ íƒ ê°€ì´ë“œ:**

| íŠ¹ì§• | FAISS | Chroma | Pinecone | Weaviate |
|------|-------|--------|----------|----------|
| **ë°°í¬** | ë¡œì»¬ | ë¡œì»¬/í´ë¼ìš°ë“œ | í´ë¼ìš°ë“œ | ë¡œì»¬/í´ë¼ìš°ë“œ |
| **í™•ì¥ì„±** | ì¤‘ê°„ | ë‚®ìŒ | ë†’ìŒ | ë†’ìŒ |
| **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰** | âŒ | âŒ | âœ… | âœ… |
| **í•„í„°ë§** | âŒ | âœ… | âœ… | âœ… |
| **ë¹„ìš©** | ë¬´ë£Œ | ë¬´ë£Œ | ìœ ë£Œ | ë¬´ë£Œ/ìœ ë£Œ |
| **ì‚¬ìš© ì‚¬ë¡€** | í”„ë¡œí† íƒ€ì… | ì¤‘ì†Œê·œëª¨ | ì—”í„°í”„ë¼ì´ì¦ˆ | ì¤‘ëŒ€ê·œëª¨ |

### 1.2.4 ë¦¬íŠ¸ë¦¬ë²„ (Retriever)

**ì—­í• **: ì‚¬ìš©ì ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰

**ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„:**
```python
# ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë³€í™˜
retriever = vectorstore.as_retriever(
    search_type="similarity",  # ìœ ì‚¬ë„ ê²€ìƒ‰
    search_kwargs={"k": 4}     # ìƒìœ„ 4ê°œ ë¬¸ì„œ
)

# ì‚¬ìš©
relevant_docs = retriever.get_relevant_documents("ì§ˆë¬¸")
```

**ê³ ê¸‰ ê²€ìƒ‰ ì „ëµ (2025ë…„ íŠ¸ë Œë“œ):**

**1) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Hybrid Search)**
```python
from langchain.retrievers import (
    EnsembleRetriever, 
    BM25Retriever
)

# ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 2

ensemble_retriever = EnsembleRetriever(
    retrievers=[retriever, bm25_retriever],
    weights=[0.5, 0.5]  # ê°ê° 50% ê°€ì¤‘ì¹˜
)
```

**2) MMR (Maximal Marginal Relevance)**
```python
# ë‹¤ì–‘ì„± ê³ ë ¤ ê²€ìƒ‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 6,
        "fetch_k": 20,        # í›„ë³´ 20ê°œ
        "lambda_mult": 0.5    # ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜
    }
)
```

**3) Self-Query Retriever**
```python
from langchain.retrievers import SelfQueryRetriever

# ë©”íƒ€ë°ì´í„° í•„í„°ë§ ìë™í™”
retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents="ê¸°ìˆ  ë¬¸ì„œ",
    metadata_field_info=[
        {"name": "author", "type": "string"},
        {"name": "year", "type": "integer"}
    ]
)
```

**4) Contextual Compression**
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
```

### 1.2.5 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Prompt Template)

**ì—­í• **: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ LLMì— ì „ë‹¬í•  í˜•ì‹ êµ¬ì„±

```python
from langchain.prompts import PromptTemplate

template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. 
ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)
```

**ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì „ëµ:**

**1) Few-shot ì˜ˆì‹œ í¬í•¨**
```python
template = """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì˜ˆì‹œ:
Q: API ì¸ì¦ ë°©ë²•ì€?
A: API í‚¤ë¥¼ í—¤ë”ì— í¬í•¨: Authorization: Bearer <key>

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""
```

**2) Chain-of-Thought**
```python
template = """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•˜ë©° ë‹µí•˜ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}

ìƒê° ê³¼ì •:
1. í•µì‹¬ ì •ë³´ íŒŒì•…:
2. ê´€ë ¨ì„± íŒë‹¨:
3. ìµœì¢… ë‹µë³€:"""
```

### 1.2.6 LLM (ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)

**ì—­í• **: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±

```python
from langchain.llms import OpenAI
from langchain.chat_models import (
    ChatOpenAI,
    ChatAnthropic,
    ChatGooglePalm
)

# GPT-4 ì‚¬ìš©
llm = ChatOpenAI(
    model_name="gpt-4-turbo-preview",
    temperature=0,  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€
    max_tokens=500
)

# Claude ì‚¬ìš©
llm = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",
    temperature=0
)
```

**2025ë…„ ê¶Œì¥ ëª¨ë¸:**

| ëª¨ë¸ | ê°•ì  | ê°€ê²© | ì¶”ì²œ ì‚¬ìš© |
|------|------|------|----------|
| **GPT-4 Turbo** | ë†’ì€ ì •í™•ë„, ê¸´ ì»¨í…ìŠ¤íŠ¸ | ğŸ’°ğŸ’°ğŸ’° | ì—”í„°í”„ë¼ì´ì¦ˆ |
| **Claude Sonnet 4.5** | ê¸´ ì»¨í…ìŠ¤íŠ¸, ì•ˆì „ì„± | ğŸ’°ğŸ’° | í”„ë¡œë•ì…˜ |
| **Gemini Pro** | ë©€í‹°ëª¨ë‹¬, ì‹¤ì‹œê°„ ê²€ìƒ‰ | ğŸ’°ğŸ’° | ìµœì‹  ì •ë³´ |
| **Llama 3 70B** | ì˜¤í”ˆì†ŒìŠ¤, ì»¤ìŠ¤í„°ë§ˆì´ì§• | ë¬´ë£Œ | ë¡œì»¬ ë°°í¬ |
| **Mistral Large** | ê· í˜•ì¡íŒ ì„±ëŠ¥ | ğŸ’° | ë¹„ìš© íš¨ìœ¨ |

### 1.2.7 ì²´ì¸/ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Chain/Orchestration)

**ì—­í• **: ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í†µí•©

**1) RetrievalQA Chain (ê¸°ë³¸)**
```python
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",      # ë¬¸ì„œë¥¼ í•œë²ˆì— ì „ë‹¬
    retriever=retriever,
    return_source_documents=True
)

# ì‚¬ìš©
result = qa_chain({"query": "RAGë€ ë¬´ì—‡ì¸ê°€?"})
print(result["result"])
print(result["source_documents"])
```

**Chain Types:**

| Type | ì„¤ëª… | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| **stuff** | ëª¨ë“  ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì „ë‹¬ | ê°„ë‹¨, ë¹ ë¦„ | ì»¨í…ìŠ¤íŠ¸ ì œí•œ |
| **map_reduce** | ê° ë¬¸ì„œ ê°œë³„ ì²˜ë¦¬ í›„ í†µí•© | ê¸´ ë¬¸ì„œ ì²˜ë¦¬ | ëŠë¦¼, ë¹„ìŒˆ |
| **refine** | ìˆœì°¨ì  ì •ì œ | ìƒì„¸í•œ ë‹µë³€ | ë§¤ìš° ëŠë¦¼ |
| **map_rerank** | ê° ë‹µë³€ì— ì ìˆ˜ ë§¤ê²¨ ì„ íƒ | í’ˆì§ˆ ë†’ìŒ | ë¹„ìš© ë†’ìŒ |

**2) ConversationalRetrievalChain (ëŒ€í™”í˜•)**
```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

# ëŒ€í™” í˜•ì‹ ì‚¬ìš©
result = conversation_chain({"question": "RAGë€?"})
result = conversation_chain({"question": "ê·¸ê²ƒì˜ ì¥ì ì€?"})
```

### 1.2.8 ë©”ëª¨ë¦¬ (Memory) - ì„ íƒì ì´ì§€ë§Œ ì¤‘ìš”

**ì—­í• **: ë‹¤íšŒì°¨ ëŒ€í™”ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ë§¥ë½ ì •ë³´ ìœ ì§€

```python
from langchain.memory import (
    ConversationBufferMemory,        # ì „ì²´ ëŒ€í™” ì €ì¥
    ConversationBufferWindowMemory,  # ìµœê·¼ kê°œë§Œ
    ConversationSummaryMemory,       # ìš”ì•½ ì €ì¥
    VectorStoreRetrieverMemory       # ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜
)

# ìœˆë„ìš° ë©”ëª¨ë¦¬ (ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ)
memory = ConversationBufferWindowMemory(
    k=5,
    memory_key="chat_history",
    return_messages=True
)
```

## 1.3 RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG System Pipeline                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data   â”‚â”€â”€â”€â”€â–¶â”‚   Loaders    â”‚â”€â”€â”€â”€â–¶â”‚ Text Splitterâ”‚
â”‚ (PDF, Web,   â”‚     â”‚(160+ sources)â”‚     â”‚  (Chunking)  â”‚
â”‚  DB, API)    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                    â”‚
                              â–¼                    â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Embeddings  â”‚â—€â”€â”€â”€â”€â”‚   Chunks     â”‚
                     â”‚   (Vectors)  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Vector Store â”‚
                     â”‚(FAISS, Pinecone)
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                            â”‚
        â–¼                                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚ User Query   â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
        â”‚                                            â”‚
        â–¼                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retriever   â”‚â”€â”€â”€(similarity search)â”€â”€â”€â”€â–¶ â”‚  Top-k Docs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                            â”‚
        â”‚                                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Prompt    â”‚
                â”‚   Template   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     LLM      â”‚
                â”‚ (GPT-4, etc) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Response   â”‚
                â”‚  + Sources   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1.4 ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„ ì˜ˆì œ

```python
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# pip install langchain chromadb sentence-transformers openai

import os
from langchain.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitters import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class RAGSystem:
    def __init__(self, data_path, model_name="gpt-4-turbo-preview"):
        self.data_path = data_path
        self.model_name = model_name
        self.vectorstore = None
        self.qa_chain = None
        
    def load_documents(self):
        """1. ë¬¸ì„œ ë¡œë“œ ë° ì²­í¬ ë¶„í• """
        print("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘...")
        loader = DirectoryLoader(
            self.data_path,
            glob="**/*.txt",
            loader_cls=TextLoader
        )
        documents = loader.load()
        
        # ì²­í¬ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = splitter.split_documents(documents)
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunks
    
    def create_embeddings(self, chunks):
        """2. ì„ë² ë”© ìƒì„± ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        print("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    def setup_retrieval_chain(self):
        """3. ë¦¬íŠ¸ë¦¬ë²„ ë° QA ì²´ì¸ ì„¤ì •"""
        print("ğŸ”— RAG ì²´ì¸ êµ¬ì„± ì¤‘...")
        
        # ë¦¬íŠ¸ë¦¬ë²„
        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )
        
        # LLM
        llm = ChatOpenAI(
            model_name=self.model_name,
            temperature=0
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
        ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
        
        ì»¨í…ìŠ¤íŠ¸:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        # QA ì²´ì¸
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
        print("âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def query(self, question):
        """4. ì§ˆì˜ ì‘ë‹µ"""
        if not self.qa_chain:
            raise RuntimeError("ë¨¼ì € setup()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")
        
        result = self.qa_chain({"query": question})
        return {
            "answer": result["result"],
            "sources": [doc.metadata for doc in result["source_documents"]]
        }
    
    def setup(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        chunks = self.load_documents()
        self.create_embeddings(chunks)
        self.setup_retrieval_chain()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = RAGSystem("./documents")
    rag.setup()
    
    # ì§ˆì˜
    response = rag.query("ì¬ìƒì—ë„ˆì§€ì˜ ì£¼ìš” ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    print("ë‹µë³€:", response["answer"])
    print("ì¶œì²˜:", response["sources"])
```

---

# 2. RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë°©ë²• {#section2}

RAG ì‹œìŠ¤í…œì€ **ê²€ìƒ‰(Retrieval)**ê³¼ **ìƒì„±(Generation)** ë‘ ê°€ì§€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë˜ë¯€ë¡œ, í‰ê°€ë„ ì´ ë‘ ë¶€ë¶„ì„ ë…ë¦½ì ìœ¼ë¡œ, ê·¸ë¦¬ê³  ì¢…ë‹¨ê°„(end-to-end)ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

## 2.1 í‰ê°€ì˜ ì„¸ ê°€ì§€ ë ˆë²¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RAG Evaluation Pyramid               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  End-to-End   â”‚
                  â”‚  (ì „ì²´ ì‹œìŠ¤í…œ)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–²
                         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Retrieval â”‚              â”‚Generationâ”‚
     â”‚ (ê²€ìƒ‰)    â”‚              â”‚  (ìƒì„±)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.2 ë…ë¦½ í‰ê°€ (Component-Level Evaluation)

### 2.2.1 ê²€ìƒ‰ í‰ê°€ (Retrieval Metrics)

**ëª©ì **: ë¦¬íŠ¸ë¦¬ë²„ê°€ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•´ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì˜ ì°¾ëŠ”ì§€ í‰ê°€

**í•µì‹¬ ë©”íŠ¸ë¦­:**

**1) Precision@k**
```
Precision@k = (ìƒìœ„ kê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ ìˆ˜) / k

ì˜ˆì‹œ:
- ìƒìœ„ 5ê°œ ë¬¸ì„œ ì¤‘ 3ê°œê°€ ê´€ë ¨ ìˆìŒ
- Precision@5 = 3/5 = 0.6
```

**ì˜ë¯¸**: ê²€ìƒ‰ ê²°ê³¼ì˜ ì •í™•ë„. ë†’ì„ìˆ˜ë¡ ë…¸ì´ì¦ˆê°€ ì ìŒ.

**2) Recall@k**
```
Recall@k = (ìƒìœ„ kê°œì—ì„œ ì°¾ì€ ê´€ë ¨ ë¬¸ì„œ ìˆ˜) / (ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ìˆ˜)

ì˜ˆì‹œ:
- ì „ì²´ ê´€ë ¨ ë¬¸ì„œ: 10ê°œ
- ìƒìœ„ 5ê°œì—ì„œ 3ê°œ ì°¾ìŒ
- Recall@5 = 3/10 = 0.3
```

**ì˜ë¯¸**: ê²€ìƒ‰ ê²°ê³¼ì˜ ì™„ì „ì„±. ë†’ì„ìˆ˜ë¡ ëˆ„ë½ì´ ì ìŒ.

**3) MRR (Mean Reciprocal Rank)**
```
MRR = 1 / (ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ìˆœìœ„)

ì˜ˆì‹œ:
- 3ë²ˆì§¸ ë¬¸ì„œê°€ ì²˜ìŒìœ¼ë¡œ ê´€ë ¨ ìˆìŒ
- MRR = 1/3 = 0.333
```

**ì˜ë¯¸**: ì²« ê´€ë ¨ ë¬¸ì„œê°€ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë‚˜ì˜¤ëŠ”ê°€. ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ.

**4) nDCG (Normalized Discounted Cumulative Gain)**
```
DCG@k = Î£(relevance_i / log2(i+1))  for i=1 to k
nDCG@k = DCG@k / Ideal_DCG@k

ì˜ˆì‹œ:
ìˆœìœ„: [1,    2,    3,    4,    5   ]
ê´€ë ¨ë„: [3,    2,    3,    0,    1   ]
DCG@5 = 3/1 + 2/1.58 + 3/2 + 0/2.32 + 1/2.58 = 6.13
```

**ì˜ë¯¸**: ìˆœìœ„ì™€ ê´€ë ¨ë„ë¥¼ ëª¨ë‘ ê³ ë ¤. ê°€ì¥ í¬ê´„ì ì¸ ì§€í‘œ.

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
from deepeval.metrics import ContextualRecallMetric, ContextualPrecisionMetric

# ê²€ìƒ‰ í‰ê°€
recall_metric = ContextualRecallMetric()
precision_metric = ContextualPrecisionMetric()

# í‰ê°€ ì‹¤í–‰
test_case = {
    "input": "ì§ˆë¬¸",
    "retrieval_context": ["ë¬¸ì„œ1", "ë¬¸ì„œ2", "ë¬¸ì„œ3"],
    "expected_output": "ì •ë‹µ"
}

recall_score = recall_metric.measure(test_case)
precision_score = precision_metric.measure(test_case)
```

**ê²€ìƒ‰ ì‹¤íŒ¨ì˜ ì£¼ìš” ì›ì¸:**
- âŒ **ë¹„íš¨ìœ¨ì  ì„ë² ë”©**: ë¶€ì ì ˆí•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
- âŒ **ì˜ëª»ëœ ì²­í¬ ì „ëµ**: ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì€ ì²­í¬
- âŒ **ë©”íƒ€ë°ì´í„° ë¶€ì¡±**: í•„í„°ë§ ì •ë³´ ì—†ìŒ

### 2.2.2 ìƒì„± í‰ê°€ (Generation Metrics)

**ëª©ì **: LLMì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ê´€ë ¨ì„± ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ì§€ í‰ê°€

**í•µì‹¬ ë©”íŠ¸ë¦­:**

**1) Faithfulness (ì¶©ì‹¤ë„)**
```
ì •ì˜: ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ê°€?
ëª©í‘œ: í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€

í‰ê°€:
- ë‹µë³€ì˜ ê° ì£¼ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì§€ë˜ëŠ”ê°€?
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì•˜ëŠ”ê°€?
```

**ì¸¡ì • ë°©ë²•:**
```python
from ragas.metrics import faithfulness

# ìë™ í‰ê°€ (LLM-as-judge)
faithfulness_score = faithfulness.evaluate(
    question="ì§ˆë¬¸",
    answer="ìƒì„±ëœ ë‹µë³€",
    contexts=["ì»¨í…ìŠ¤íŠ¸1", "ì»¨í…ìŠ¤íŠ¸2"]
)
# ì ìˆ˜: 0.0 (ì™„ì „íˆ ì§€ì–´ëƒ„) ~ 1.0 (ì™„ì „íˆ ì¶©ì‹¤)
```

**2) Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)**
```
ì •ì˜: ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ”ê°€?
ëª©í‘œ: ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ì •ë³´ ì œê±°

í‰ê°€:
- ì§ˆë¬¸ì˜ í•µì‹¬ì— ëŒ€ë‹µí•˜ëŠ”ê°€?
- ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
```

**ì¸¡ì •:**
```python
from ragas.metrics import answer_relevancy

relevancy_score = answer_relevancy.evaluate(
    question="RAGì˜ ì¥ì ì€?",
    answer="RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤..."
)
```

**3) Citation Coverage (ì¸ìš© ë²”ìœ„)**
```
ì •ì˜: ì£¼ì¥ë“¤ì´ ì ì ˆíˆ ì¶œì²˜ë¥¼ ë°íˆëŠ”ê°€?

í‰ê°€:
- ê° ì‚¬ì‹¤ì  ì£¼ì¥ì— ì¸ìš©ì´ ìˆëŠ”ê°€?
- ì¸ìš©ì´ ì •í™•í•œ ì¶œì²˜ë¥¼ ê°€ë¦¬í‚¤ëŠ”ê°€?
```

**4) Hallucination Rate (í™˜ê° ë¹„ìœ¨)**
```
í™˜ê° ë¹„ìœ¨ = (ì§€ì–´ë‚¸ ì •ë³´ ìˆ˜) / (ì „ì²´ ì£¼ì¥ ìˆ˜)

ì˜ˆì‹œ:
- ì´ 10ê°œ ì£¼ì¥ ì¤‘ 2ê°œê°€ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ
- í™˜ê° ë¹„ìœ¨ = 2/10 = 0.2 (20%)
```

**ì¸¡ì • ë°©ë²•:**
```python
# Named Entity ì¼ì¹˜ í™•ì¸
def hallucination_rate(answer, contexts):
    answer_entities = extract_entities(answer)
    context_entities = extract_entities(" ".join(contexts))
    
    hallucinated = [
        e for e in answer_entities 
        if e not in context_entities
    ]
    
    return len(hallucinated) / len(answer_entities)
```

**ìƒì„± ì‹¤íŒ¨ì˜ ì£¼ìš” ì›ì¸:**
- âŒ **ì»¨í…ìŠ¤íŠ¸ ë¬´ì‹œ**: LLMì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œëŒ€ë¡œ í™œìš© ì•ˆ í•¨
- âŒ **í”„ë¡¬í”„íŠ¸ ë¬¸ì œ**: ì§€ì‹œì‚¬í•­ì´ ë¶ˆëª…í™•
- âŒ **ëª¨ë¸ í•œê³„**: ë„ˆë¬´ ì‘ì€ ëª¨ë¸ ì‚¬ìš©

## 2.3 ì¢…ë‹¨ê°„ í‰ê°€ (End-to-End Evaluation)

**ëª©ì **: ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜ ê´€ì ì—ì„œ ì „ì²´ ì‹œìŠ¤í…œ í‰ê°€

### 2.3.1 E2E í•µì‹¬ ë©”íŠ¸ë¦­

**1) Correctness (ì •í™•ì„±)**
```
ì •ì˜: ìµœì¢… ë‹µë³€ì´ ì‚¬ì‹¤ì ìœ¼ë¡œ ë§ëŠ”ê°€?

ì¸¡ì •:
- ìˆ˜ë™: ì‚¬ëŒì´ ì •ë‹µê³¼ ë¹„êµ
- ìë™: LLM-as-judge í™œìš©
```

**êµ¬í˜„:**
```python
from deepeval.metrics import AnswerRelevancyMetric

metric = AnswerRelevancyMetric(threshold=0.7)

test_case = {
    "input": "ì„¸ì¢…ëŒ€ì™•ì´ í•œê¸€ì„ ë§Œë“  ì—°ë„ëŠ”?",
    "actual_output": "1443ë…„ì…ë‹ˆë‹¤.",
    "expected_output": "1443ë…„",
    "context": ["ì„¸ì¢…ëŒ€ì™•ì€ 1443ë…„ì—..."]
}

result = metric.measure(test_case)
```

**2) Factuality (ì‚¬ì‹¤ì„±)**
```
ì •ì˜: ë‹µë³€ì´ ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ëŠ”ê°€?

ì°¨ì´ì :
- Correctness: ì •ë‹µê³¼ì˜ ì¼ì¹˜
- Factuality: ì™¸ë¶€ ì§€ì‹ê³¼ì˜ ì¼ì¹˜
```

**3) Latency (ì‘ë‹µ ì‹œê°„)**
```
ì¸¡ì • í•­ëª©:
- ê²€ìƒ‰ ì‹œê°„
- LLM ì¶”ë¡  ì‹œê°„
- ì „ì²´ ì‘ë‹µ ì‹œê°„

ëª©í‘œ:
- ëŒ€í™”í˜•: < 2ì´ˆ
- ë¶„ì„í˜•: < 5ì´ˆ
```

**ì¸¡ì •:**
```python
import time

start = time.time()
response = qa_chain({"query": "ì§ˆë¬¸"})
latency = time.time() - start

print(f"ì‘ë‹µ ì‹œê°„: {latency:.2f}ì´ˆ")
```

**4) Cost per Query (ì¿¼ë¦¬ë‹¹ ë¹„ìš©)**
```
ë¹„ìš© = ì„ë² ë”© ë¹„ìš© + LLM ë¹„ìš©

ì˜ˆì‹œ (GPT-4):
- ê²€ìƒ‰: $0.001
- ìƒì„± (500 í† í°): $0.015
- ì´: $0.016 per query
```

**5) User Satisfaction Score**
```
ì¸¡ì • ë°©ë²•:
- ğŸ‘/ğŸ‘ í”¼ë“œë°±
- 1-5ì  í‰ê°€
- ëŒ€í™” ì™„ë£Œìœ¨
```

### 2.3.2 ì¢…ë‹¨ê°„ vs ë…ë¦½ í‰ê°€ ë¹„êµ

| ì°¨ì› | ë…ë¦½ í‰ê°€ | ì¢…ë‹¨ê°„ í‰ê°€ |
|------|----------|------------|
| **ì´ˆì ** | ê°œë³„ ì»´í¬ë„ŒíŠ¸ | ì „ì²´ ì‹œìŠ¤í…œ |
| **ì„¸ë¶„ì„±** | ë†’ìŒ (ë””ë²„ê¹… ìš©ì´) | ë‚®ìŒ |
| **ì‚¬ìš©ì ê´€ì ** | ê°„ì ‘ì  | ì§ì ‘ì  |
| **ìµœì í™”** | ì»´í¬ë„ŒíŠ¸ë³„ ê°œì„  | ì „ì²´ì  ê· í˜• |
| **ë¹„ìš©** | ìƒëŒ€ì  ì €ë ´ | ìƒëŒ€ì  ë¹„ìŒˆ |
| **ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ |

**ì–¸ì œ ë¬´ì—‡ì„ ì‚¬ìš©?**

```
ê°œë°œ ì´ˆê¸°: ë…ë¦½ í‰ê°€ ì¤‘ì‹¬
â†’ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„  (Recall, Precision)
â†’ ìƒì„± í’ˆì§ˆ ê°œì„  (Faithfulness)

ê°œë°œ í›„ë°˜: ì¢…ë‹¨ê°„ í‰ê°€ ì¶”ê°€
â†’ ì‹¤ì œ ì‚¬ìš©ì„± ê²€ì¦
â†’ ë¹„ìš©, ì†ë„ ìµœì í™”

í”„ë¡œë•ì…˜: ë‘ ë°©ë²• ë³‘í–‰
â†’ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (E2E)
â†’ ë¬¸ì œ ì§„ë‹¨ (Component)
```

## 2.4 í‰ê°€ í”„ë ˆì„ì›Œí¬ ë° ë„êµ¬

### 2.4.1 Ragas (RAG Assessment)

**íŠ¹ì§•**: RAG ì „ìš© í‰ê°€ í”„ë ˆì„ì›Œí¬

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset

# í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„
data = {
    "question": ["Q1", "Q2", "Q3"],
    "answer": ["A1", "A2", "A3"],
    "contexts": [["C1"], ["C2"], ["C3"]],
    "ground_truths": ["GT1", "GT2", "GT3"]
}
dataset = Dataset.from_dict(data)

# ì¢…í•© í‰ê°€
result = evaluate(
    dataset,
    metrics=[
        context_precision,
        context_recall,
        faithfulness,
        answer_relevancy
    ]
)

print(result)
# {
#   'context_precision': 0.85,
#   'context_recall': 0.90,
#   'faithfulness': 0.88,
#   'answer_relevancy': 0.92
# }
```

### 2.4.2 DeepEval

**íŠ¹ì§•**: í”„ë¡œë•ì…˜ ì¤‘ì‹¬ í‰ê°€ í”Œë«í¼

```python
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    ContextualRecallMetric
)

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
test_case = LLMTestCase(
    input="RAGë€ ë¬´ì—‡ì¸ê°€?",
    actual_output="ê²€ìƒ‰ ì¦ê°• ìƒì„±...",
    expected_output="Retrieval-Augmented Generation...",
    retrieval_context=["ì»¨í…ìŠ¤íŠ¸1", "ì»¨í…ìŠ¤íŠ¸2"]
)

# ë©”íŠ¸ë¦­ ì •ì˜
metrics = [
    AnswerRelevancyMetric(threshold=0.7),
    FaithfulnessMetric(threshold=0.8),
    ContextualRecallMetric(threshold=0.7)
]

# CI/CDì—ì„œ ìë™ í…ŒìŠ¤íŠ¸
assert_test(test_case, metrics)
```

### 2.4.3 LangSmith (LangChain ê³µì‹)

**íŠ¹ì§•**: íŠ¸ë ˆì´ì‹±, ëª¨ë‹ˆí„°ë§, í‰ê°€ í†µí•©

```python
from langsmith import Client

client = Client()

# íŠ¸ë ˆì´ìŠ¤ ê¸°ë¡
with client.trace("RAG Query") as run:
    result = qa_chain({"query": "ì§ˆë¬¸"})

# í‰ê°€ ì‹¤í–‰
client.evaluate(
    dataset_name="test_dataset",
    evaluators=[
        "qa_evaluator",
        "embedding_distance"
    ]
)
```

### 2.4.4 í‰ê°€ ë„êµ¬ ë¹„êµ

| ë„êµ¬ | ê°•ì  | ì•½ì  | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|------|------|------|--------------|
| **Ragas** | RAG íŠ¹í™”, ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ | ë¬¸ì„œ ë¶€ì¡± | ì—°êµ¬, ë²¤ì¹˜ë§ˆí¬ |
| **DeepEval** | CI/CD í†µí•©, pytest ìŠ¤íƒ€ì¼ | ì„¤ì • ë³µì¡ | í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ |
| **LangSmith** | LangChain í†µí•©, ì‹œê°í™” | ìœ ë£Œ | ê°œë°œ, ë””ë²„ê¹… |
| **TruLens** | ì„¤ëª…ê°€ëŠ¥ì„±, ì¸ì‚¬ì´íŠ¸ | ëŠë¦¼ | ë¶„ì„, ê°œì„  |

## 2.5 í‰ê°€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 2.5.1 í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶•

**1) Golden Dataset (ìˆ˜ë™ ë ˆì´ë¸”ë§)**
```python
golden_dataset = [
    {
        "question": "RAGì˜ ì£¼ìš” ì¥ì ì€?",
        "ground_truth": "ìµœì‹  ì •ë³´ ì ‘ê·¼, í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ",
        "relevant_docs": ["doc_123", "doc_456"]
    },
    # ìµœì†Œ 100ê°œ ì´ìƒ ê¶Œì¥
]
```

**2) í•©ì„± ë°ì´í„° ìƒì„±**
```python
from ragas.testset.generator import TestsetGenerator

# ë¬¸ì„œì—ì„œ ìë™ìœ¼ë¡œ Q&A ìƒì„±
generator = TestsetGenerator.from_langchain(
    llm=llm,
    embeddings=embeddings
)

testset = generator.generate_with_documents(
    documents=chunks,
    test_size=100
)
```

### 2.5.2 ì§€ì†ì  í‰ê°€ íŒŒì´í”„ë¼ì¸

```python
# CI/CD í†µí•© ì˜ˆì‹œ (GitHub Actions)
def test_rag_system():
    """
    ë§¤ ë°°í¬ ì „ ìë™ ì‹¤í–‰
    """
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_cases = load_test_dataset()
    
    # 2. í‰ê°€ ì‹¤í–‰
    results = []
    for case in test_cases:
        response = rag_system.query(case["question"])
        
        # ê° ë©”íŠ¸ë¦­ ê³„ì‚°
        faithfulness = calculate_faithfulness(
            response, case["ground_truth"]
        )
        latency = measure_latency()
        cost = estimate_cost()
        
        results.append({
            "faithfulness": faithfulness,
            "latency": latency,
            "cost": cost
        })
    
    # 3. ì„ê³„ê°’ ê²€ì¦
    avg_faithfulness = np.mean([r["faithfulness"] for r in results])
    assert avg_faithfulness > 0.8, "Faithfulness too low!"
    
    avg_latency = np.mean([r["latency"] for r in results])
    assert avg_latency < 3.0, "Latency too high!"
```

### 2.5.3 A/B í…ŒìŠ¤íŠ¸

```python
# ë‘ RAG ë²„ì „ ë¹„êµ
def ab_test(version_a, version_b, queries):
    """
    A/B í…ŒìŠ¤íŠ¸: ì‹¤ì œ ì‚¬ìš©ì ì¿¼ë¦¬ë¡œ ë‘ ë²„ì „ ë¹„êµ
    """
    results = {"A": [], "B": []}
    
    for query in queries:
        # ë¬´ì‘ìœ„ í• ë‹¹
        version = random.choice(["A", "B"])
        system = version_a if version == "A" else version_b
        
        response = system.query(query)
        user_rating = collect_user_feedback()
        
        results[version].append({
            "response": response,
            "rating": user_rating
        })
    
    # í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
    p_value = ttest_ind(results["A"], results["B"])
    
    return {
        "winner": "A" if np.mean(results["A"]) > np.mean(results["B"]) else "B",
        "p_value": p_value
    }
```

---

# 3. RAG Agent: ê°œë…ê³¼ êµ¬í˜„ {#section3}

## 3.1 Agentë€ ë¬´ì—‡ì¸ê°€?

**Agent**ëŠ” ë‹¨ìˆœí•œ ì…ë ¥-ì¶œë ¥ ëª¨ë¸ì„ ë„˜ì–´, **ììœ¨ì ìœ¼ë¡œ ê²°ì •í•˜ê³  í–‰ë™í•˜ëŠ” AI ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

### 3.1.1 ê¸°ë³¸ RAG vs Agentic RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Basic RAG (ì •ì )                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query â†’ [ê³ ì •ëœ ê²€ìƒ‰] â†’ ë¬¸ì„œ â†’ LLM â†’ ë‹µë³€


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Agentic RAG (ë™ì , ììœ¨ì )               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query â†’ [Agent íŒë‹¨] â”€â”¬â†’ ê²€ìƒ‰ í•„ìš”? â†’ ë²¡í„° DB ê²€ìƒ‰
                      â”œâ†’ ê³„ì‚° í•„ìš”? â†’ ê³„ì‚°ê¸° ì‹¤í–‰
                      â”œâ†’ ìµœì‹  ì •ë³´? â†’ ì›¹ ê²€ìƒ‰
                      â”œâ†’ ì½”ë“œ ì‹¤í–‰? â†’ Python ì¸í„°í”„ë¦¬í„°
                      â””â†’ ì¶©ë¶„í•œ ì •ë³´? â†’ ë‹µë³€ ìƒì„±
                             â†“
                     [ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šìœ¼ë©´ ë‹¤ì‹œ ë„êµ¬ ì‚¬ìš©]
                             â†“
                          ìµœì¢… ë‹µë³€
```

### 3.1.2 Agentì˜ í•µì‹¬ íŠ¹ì§•

**1) Autonomy (ììœ¨ì„±)**
- ìŠ¤ìŠ¤ë¡œ ë¬´ì—‡ì„ í• ì§€ ê²°ì •
- ê³ ì •ëœ íë¦„ì´ ì•„ë‹Œ ë™ì  ê³„íš

**2) Tool Use (ë„êµ¬ ì‚¬ìš©)**
- ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš©
- ê³„ì‚°ê¸°, ê²€ìƒ‰ ì—”ì§„, API, ë°ì´í„°ë² ì´ìŠ¤ ë“±

**3) Reasoning (ì¶”ë¡ )**
- Chain-of-Thought: ë‹¨ê³„ì  ì‚¬ê³ 
- ReAct: Reasoning + Acting ê²°í•©

**4) Memory (ê¸°ì–µ)**
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€
- ê³¼ê±° ì•¡ì…˜ í•™ìŠµ

## 3.2 Agentì˜ ì‘ë™ ì›ë¦¬: ReAct íŒ¨í„´

**ReAct (Reasoning and Acting)** íŒ¨í„´ì€ 2025ë…„ Agentì˜ í‘œì¤€ì…ë‹ˆë‹¤.

### 3.2.1 ReAct ì‚¬ì´í´

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ReAct Agent Loop                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ Thought (ì‚¬ê³ )
   â†“
   "ì‚¬ìš©ìê°€ ìµœì‹  ì£¼ê°€ë¥¼ ë¬¼ì–´ë´¤ë‹¤. 
    ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í•„ìš”í•˜ë¯€ë¡œ ì£¼ì‹ APIë¥¼ í˜¸ì¶œí•´ì•¼ê² ë‹¤."

2ï¸âƒ£ Action (í–‰ë™)
   â†“
   Tool: stock_price_api("AAPL")
   
3ï¸âƒ£ Observation (ê´€ì°°)
   â†“
   "AAPLì˜ í˜„ì¬ê°€ëŠ” $185.23ì…ë‹ˆë‹¤."
   
4ï¸âƒ£ Thought (ì¬ì‚¬ê³ )
   â†“
   "ê°€ê²© ì •ë³´ë¥¼ ì–»ì—ˆë‹¤. 
    ì´ì œ ì‚¬ìš©ìì—ê²Œ ë‹µë³€í•  ìˆ˜ ìˆë‹¤."

5ï¸âƒ£ Final Answer (ìµœì¢… ë‹µë³€)
   â†“
   "ì• í”Œì˜ í˜„ì¬ ì£¼ê°€ëŠ” $185.23ì…ë‹ˆë‹¤."
```

**ì½”ë“œ ì˜ˆì‹œ:**
```python
from langchain.agents import create_react_agent
from langchain.tools import Tool
from langchain.prompts import PromptTemplate

# ë„êµ¬ ì •ì˜
def stock_price(ticker: str) -> str:
    """ì£¼ì‹ ê°€ê²© ì¡°íšŒ (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ)"""
    return f"{ticker}ì˜ í˜„ì¬ê°€: $185.23"

tools = [
    Tool(
        name="StockPrice",
        func=stock_price,
        description="ì£¼ì‹ í‹°ì»¤ì˜ í˜„ì¬ ê°€ê²© ì¡°íšŒ. ì…ë ¥: í‹°ì»¤ ì‹¬ë³¼"
    )
]

# ReAct í”„ë¡¬í”„íŠ¸
prompt = PromptTemplate.from_template("""
Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought: {agent_scratchpad}
""")

# Agent ìƒì„±
agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=prompt
)

# ì‹¤í–‰
from langchain.agents import AgentExecutor

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True
)

result = agent_executor.invoke({
    "input": "ì• í”Œ ì£¼ì‹ ê°€ê²©ì´ ì–¼ë§ˆì•¼?"
})
```

**ì‹¤í–‰ ë¡œê·¸:**
```
> Entering new AgentExecutor chain...
Thought: ì‚¬ìš©ìê°€ ì• í”Œ ì£¼ê°€ë¥¼ ë¬¼ì–´ë´¤ë‹¤. ì‹¤ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤.
Action: StockPrice
Action Input: AAPL
Observation: AAPLì˜ í˜„ì¬ê°€: $185.23
Thought: ê°€ê²© ì •ë³´ë¥¼ ì–»ì—ˆë‹¤. ì´ì œ ë‹µë³€í•  ìˆ˜ ìˆë‹¤.
Final Answer: ì• í”Œì˜ í˜„ì¬ ì£¼ê°€ëŠ” $185.23ì…ë‹ˆë‹¤.

> Finished chain.
```

## 3.3 RAG Agent êµ¬í˜„

### 3.3.1 ë„êµ¬ ì •ì˜

```python
from langchain.tools import tool

@tool
def vector_search(query: str) -> str:
    """ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰. ì§€ì‹ ë² ì´ìŠ¤ ì§ˆë¬¸ì— ì‚¬ìš©."""
    docs = vectorstore.similarity_search(query, k=3)
    return "\n".join([doc.page_content for doc in docs])

@tool
def web_search(query: str) -> str:
    """ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰. ìµœì‹  ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©."""
    # ì‹¤ì œë¡œëŠ” SerpAPI, Google Custom Search ë“± ì‚¬ìš©
    return "ì›¹ ê²€ìƒ‰ ê²°ê³¼..."

@tool
def calculator(expression: str) -> float:
    """ìˆ˜í•™ ê³„ì‚° ìˆ˜í–‰. ì…ë ¥: Python í‘œí˜„ì‹"""
    return eval(expression)

tools = [vector_search, web_search, calculator]
```

### 3.3.2 ì™„ì „í•œ RAG Agent

```python
from langchain.agents import create_openai_tools_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory

# í”„ë¡¬í”„íŠ¸
prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    
ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- vector_search: ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ (ë‚´ë¶€ ë¬¸ì„œ)
- web_search: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ (ìµœì‹  ì •ë³´)
- calculator: ìˆ˜í•™ ê³„ì‚°

ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”."""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Agent ìƒì„±
agent = create_openai_tools_agent(
    llm=llm,
    tools=tools,
    prompt=prompt
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True,
    max_iterations=5  # ë¬´í•œ ë£¨í”„ ë°©ì§€
)

# ì‚¬ìš©
response = agent_executor.invoke({
    "input": "ìš°ë¦¬ íšŒì‚¬ì˜ 2024 ë§¤ì¶œì€? ê·¸ë¦¬ê³  ì „ë…„ ëŒ€ë¹„ ì¦ê°€ìœ¨ì„ ê³„ì‚°í•´ì¤˜"
})
```

**Agentì˜ ì‚¬ê³  ê³¼ì •:**
```
ì‚¬ê³ : íšŒì‚¬ ë§¤ì¶œ â†’ ë‚´ë¶€ ë¬¸ì„œì— ìˆì„ ê²ƒ â†’ vector_search ì‚¬ìš©
í–‰ë™: vector_search("2024 ë§¤ì¶œ")
ê´€ì°°: "2024ë…„ ë§¤ì¶œ: 500ì–µ ì›, 2023ë…„ ë§¤ì¶œ: 400ì–µ ì›"

ì‚¬ê³ : ì¦ê°€ìœ¨ ê³„ì‚° í•„ìš” â†’ calculator ì‚¬ìš©
í–‰ë™: calculator("(500-400)/400 * 100")
ê´€ì°°: "25.0"

ì‚¬ê³ : ì •ë³´ê°€ ì¶©ë¶„í•˜ë‹¤. ë‹µë³€ ìƒì„±
ë‹µë³€: "2024ë…„ ë§¤ì¶œì€ 500ì–µ ì›ì´ë©°, ì „ë…„ ëŒ€ë¹„ 25% ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
```

## 3.4 ê³ ê¸‰ Agent íŒ¨í„´

### 3.4.1 Supervisor Agent (ê°ë…ì íŒ¨í„´)

ì—¬ëŸ¬ ì „ë¬¸ agentë¥¼ ê´€ë¦¬í•˜ëŠ” ìƒìœ„ agent

```python
from langgraph.prebuilt import create_supervisor

# ì „ë¬¸ agents
data_agent = create_agent(llm, [sql_tool, analytics_tool])
research_agent = create_agent(llm, [web_search, summarize_tool])
code_agent = create_agent(llm, [python_tool, github_tool])

# Supervisor
supervisor = create_supervisor(
    llm=llm,
    agents=[
        ("DataAnalyst", data_agent),
        ("Researcher", research_agent),
        ("Developer", code_agent)
    ]
)

# ì‚¬ìš©ì ìš”ì²­ì„ ì ì ˆí•œ agentì— ìœ„ì„
result = supervisor.invoke({
    "input": "ìµœê·¼ AI ë…¼ë¬¸ì„ ì°¾ê³ , í•µì‹¬ ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ì½”ë“œë¡œ êµ¬í˜„í•´ì¤˜"
})
# â†’ Researcher â†’ DataAnalyst â†’ Developer ìˆœì„œë¡œ ì‹¤í–‰
```

### 3.4.2 Swarm Agent (í˜‘ì—… íŒ¨í„´)

Agentë“¤ì´ ì„œë¡œ í˜‘ë ¥í•˜ë©° ì‘ì—… ì „ë‹¬

```python
from langgraph.prebuilt import create_swarm

# ê° agentê°€ ë‹¤ë¥¸ agentì—ê²Œ ì‘ì—… ì „ë‹¬ ê°€ëŠ¥
agents = {
    "retriever": retrieval_agent,
    "analyzer": analysis_agent,
    "writer": writing_agent
}

swarm = create_swarm(agents)

# ì‘ì—… íë¦„ì´ ë™ì ìœ¼ë¡œ ê²°ì •ë¨
# retriever â†’ analyzer â†’ writer
# ë˜ëŠ” retriever â†’ writer (ê°„ë‹¨í•œ ê²½ìš°)
```

### 3.4.3 Planning Agent (ê³„íš ìˆ˜ë¦½)

ë³µì¡í•œ ì‘ì—…ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´

```python
from langchain.agents import PlanAndExecute

# Plan: ì „ì²´ ê³„íš ìˆ˜ë¦½
# Execute: ê° ë‹¨ê³„ ì‹¤í–‰
agent = PlanAndExecute.from_llm_and_tools(
    llm=llm,
    tools=tools
)

result = agent.invoke({
    "input": """
    ìš°ë¦¬ ì œí’ˆì˜ ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜:
    1. ê²½ìŸì‚¬ ì¡°ì‚¬
    2. ì‹œì¥ ê·œëª¨ ê³„ì‚°
    3. SWOT ë¶„ì„
    4. ìµœì¢… ë³´ê³ ì„œ ì‘ì„±
    """
})

# Agentê°€ ìë™ìœ¼ë¡œ 4ë‹¨ê³„ ê³„íš ìˆ˜ë¦½ â†’ ìˆœì°¨ ì‹¤í–‰
```

## 3.5 LangGraph: ê³ ê¸‰ Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

**LangGraph**ëŠ” ë³µì¡í•œ agent ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    messages: List[str]
    current_task: str
    completed_tasks: List[str]

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("researcher", research_node)
workflow.add_node("analyzer", analyze_node)
workflow.add_node("writer", write_node)

# ì—£ì§€ (íë¦„) ì •ì˜
workflow.add_edge("researcher", "analyzer")
workflow.add_edge("analyzer", "writer")
workflow.add_edge("writer", END)

# ì¡°ê±´ë¶€ ì—£ì§€
def should_continue(state):
    if "ì¶”ê°€ ì¡°ì‚¬ í•„ìš”" in state["messages"][-1]:
        return "researcher"
    return "writer"

workflow.add_conditional_edges(
    "analyzer",
    should_continue,
    {
        "researcher": "researcher",
        "writer": "writer"
    }
)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile()

# ì‹¤í–‰
result = app.invoke({
    "messages": ["AI íŠ¸ë Œë“œ ë³´ê³ ì„œ ì‘ì„±"],
    "current_task": "research",
    "completed_tasks": []
})
```

**ê·¸ë˜í”„ ì‹œê°í™”:**
```
    [Start]
       â†“
  [Researcher] â†â”€â”€â”
       â†“          â”‚
   [Analyzer] â”€â”€â”€â”€â”¤
       â†“          â”‚
    [Writer]      â”‚
       â†“          â”‚
     [End] â”€â”€â”€â”€â”€â”€â”€â”˜
     (ì¡°ê±´: ì¶”ê°€ ì¡°ì‚¬ í•„ìš”)
```

## 3.6 Agentic RAG: ì‹¤ì „ ì˜ˆì œ

```python
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.tools import tool
import requests

# ë„êµ¬ 1: ë²¡í„° ê²€ìƒ‰
@tool
def knowledge_base_search(query: str) -> str:
    """íšŒì‚¬ ë‚´ë¶€ ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰"""
    docs = vectorstore.similarity_search(query, k=3)
    return "\n".join([f"- {doc.page_content}" for doc in docs])

# ë„êµ¬ 2: ì›¹ ê²€ìƒ‰
@tool
def internet_search(query: str) -> str:
    """ìµœì‹  ì •ë³´ë¥¼ ìœ„í•œ ì¸í„°ë„· ê²€ìƒ‰"""
    # SerpAPI ì‚¬ìš©
    results = requests.get(
        "https://serpapi.com/search",
        params={"q": query, "api_key": "..."}
    ).json()
    
    snippets = [r["snippet"] for r in results["organic_results"][:3]]
    return "\n".join(snippets)

# ë„êµ¬ 3: SQL ì‹¤í–‰
@tool
def database_query(sql: str) -> str:
    """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰"""
    # ì‹¤ì œë¡œëŠ” ì•ˆì „í•œ ì¿¼ë¦¬ ì‹¤í–‰ í•„ìš”
    import sqlite3
    conn = sqlite3.connect("company.db")
    result = conn.execute(sql).fetchall()
    return str(result)

# Agent êµ¬ì„±
tools = [knowledge_base_search, internet_search, database_query]

prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ íšŒì‚¬ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:
- íšŒì‚¬ ì •ì±…, ë¬¸ì„œ â†’ knowledge_base_search
- ìµœì‹  ë‰´ìŠ¤, ì‹œì¥ ë™í–¥ â†’ internet_search  
- ë§¤ì¶œ, ê³ ê° ë°ì´í„° â†’ database_query

ì—¬ëŸ¬ ë„êµ¬ë¥¼ ì¡°í•©í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True
)

# ë³µì¡í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
response = agent_executor.invoke({
    "input": """
    ìš°ë¦¬ íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì„ ì•Œë ¤ì£¼ê³ ,
    ì˜¬í•´ ì§ì›ë‹¹ í‰ê·  íœ´ê°€ ì‚¬ìš©ì¼ìˆ˜ë¥¼ ê³„ì‚°í•œ ë’¤,
    ì—…ê³„ í‰ê· ê³¼ ë¹„êµí•´ì¤˜.
    """
})
```

**Agent ì‹¤í–‰ ë¡œê·¸:**
```
> Entering new AgentExecutor chain...

Thought: 3ë‹¨ê³„ ì‘ì—…ì´ í•„ìš”í•˜ë‹¤
  1. íœ´ê°€ ì •ì±… â†’ knowledge_base_search
  2. í‰ê·  ì‚¬ìš©ì¼ìˆ˜ â†’ database_query
  3. ì—…ê³„ í‰ê·  â†’ internet_search

Action: knowledge_base_search
Action Input: "íœ´ê°€ ì •ì±…"
Observation: ì§ì›ì€ ì—°ê°„ 15ì¼ì˜ ìœ ê¸‰íœ´ê°€ë¥¼ ë°›ìŠµë‹ˆë‹¤...

Action: database_query
Action Input: "SELECT AVG(vacation_days_used) FROM employees WHERE year=2024"
Observation: [(12.3,)]

Action: internet_search
Action Input: "ëŒ€í•œë¯¼êµ­ í‰ê·  ì—°ì°¨ ì‚¬ìš©ì¼ìˆ˜ 2024"
Observation: 2024ë…„ í‰ê·  ì—°ì°¨ ì‚¬ìš©ì¼ìˆ˜ëŠ” 11.2ì¼...

Thought: ëª¨ë“  ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆë‹¤. ì¢…í•© ë‹µë³€ ì‘ì„±

Final Answer:
ìš°ë¦¬ íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì€ ì—°ê°„ 15ì¼ì˜ ìœ ê¸‰íœ´ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ì˜¬í•´ ì§ì›ë‹¹ í‰ê·  íœ´ê°€ ì‚¬ìš©ì¼ìˆ˜ëŠ” 12.3ì¼ì…ë‹ˆë‹¤.
ì´ëŠ” ì—…ê³„ í‰ê·  11.2ì¼ë³´ë‹¤ 1.1ì¼ ë§ì€ ìˆ˜ì¹˜ë¡œ, 
ì§ì›ë“¤ì˜ ì¼ê³¼ ì‚¶ì˜ ê· í˜•ì´ ì˜ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.

> Finished chain.
```

---

# 4. ì‹¤ì „ êµ¬í˜„ ì˜ˆì œ {#section4}

## 4.1 í”„ë¡œë•ì…˜ê¸‰ RAG ì‹œìŠ¤í…œ

```python
import os
from typing import List, Dict, Any
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitters import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.callbacks import get_openai_callback
import pinecone

class ProductionRAGSystem:
    """í”„ë¡œë•ì…˜ í™˜ê²½ìš© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        data_path: str,
        index_name: str = "rag-production",
        model: str = "gpt-4-turbo-preview"
    ):
        self.data_path = data_path
        self.index_name = index_name
        self.model = model
        
        # ì´ˆê¸°í™”
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.chain = None
        self.memory = None
        
    def setup_embeddings(self):
        """ì„ë² ë”© ì„¤ì •"""
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            dimensions=1024  # ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ì°¨ì› ì¶•ì†Œ
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
    
    def load_and_chunk_documents(self) -> List:
        """ë¬¸ì„œ ë¡œë“œ ë° ì²­í¬ ë¶„í•  (ê°œì„ ëœ ì „ëµ)"""
        print("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘...")
        
        # ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›
        loaders = {
            "**/*.txt": TextLoader,
            "**/*.md": TextLoader,
            "**/*.pdf": PyPDFLoader,
        }
        
        all_docs = []
        for glob_pattern, loader_cls in loaders.items():
            loader = DirectoryLoader(
                self.data_path,
                glob=glob_pattern,
                loader_cls=loader_cls
            )
            all_docs.extend(loader.load())
        
        # ì˜ë¯¸ë¡ ì  ì²­í¬ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""],
            add_start_index=True  # ì›ë³¸ ìœ„ì¹˜ ì¶”ì 
        )
        
        chunks = splitter.split_documents(all_docs)
        
        # ë©”íƒ€ë°ì´í„° ê°•í™”
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = f"chunk_{i}"
            chunk.metadata["char_count"] = len(chunk.page_content)
        
        print(f"âœ… {len(all_docs)}ê°œ ë¬¸ì„œ â†’ {len(chunks)}ê°œ ì²­í¬")
        return chunks
    
    def setup_vectorstore(self, chunks: List):
        """ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • (Pinecone)"""
        print("ğŸ”¢ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        # Pinecone ì´ˆê¸°í™”
        pinecone.init(
            api_key=os.getenv("PINECONE_API_KEY"),
            environment=os.getenv("PINECONE_ENV")
        )
        
        # ì¸ë±ìŠ¤ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´)
        if self.index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=self.index_name,
                dimension=1024,
                metric="cosine"
            )
        
        # ë²¡í„° ìŠ¤í† ì–´
        self.vectorstore = Pinecone.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            index_name=self.index_name
        )
        
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    def setup_llm(self):
        """LLM ì„¤ì •"""
        self.llm = ChatOpenAI(
            model_name=self.model,
            temperature=0,  # ì¼ê´€ì„±
            streaming=True  # ì‹¤ì‹œê°„ ì‘ë‹µ
        )
        print("âœ… LLM ì„¤ì • ì™„ë£Œ")
    
    def setup_retrieval_chain(self):
        """ê²€ìƒ‰ ì²´ì¸ ì„¤ì •"""
        print("ğŸ”— RAG ì²´ì¸ êµ¬ì„± ì¤‘...")
        
        # ê³ ê¸‰ ë¦¬íŠ¸ë¦¬ë²„ (MMR + Contextual Compression)
        from langchain.retrievers import ContextualCompressionRetriever
        from langchain.retrievers.document_compressors import LLMChainExtractor
        
        base_retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 6,
                "fetch_k": 20,
                "lambda_mult": 0.5
            }
        )
        
        compressor = LLMChainExtractor.from_llm(self.llm)
        retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )
        
        # ë©”ëª¨ë¦¬ (ìµœê·¼ 10í„´)
        self.memory = ConversationBufferWindowMemory(
            k=10,
            memory_key="chat_history",
            output_key="answer",
            return_messages=True
        )
        
        # ëŒ€í™”í˜• ì²´ì¸
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            return_source_documents=True,
            verbose=False
        )
        
        print("âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def query(
        self,
        question: str,
        stream: bool = False
    ) -> Dict[str, Any]:
        """ì§ˆì˜ ì‘ë‹µ"""
        if not self.chain:
            raise RuntimeError("ë¨¼ì € setup()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ë¹„ìš© ì¶”ì 
        with get_openai_callback() as cb:
            result = self.chain({"question": question})
            
            response = {
                "answer": result["answer"],
                "sources": [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata
                    }
                    for doc in result["source_documents"]
                ],
                "cost": {
                    "tokens": cb.total_tokens,
                    "cost_usd": cb.total_cost
                }
            }
        
        return response
    
    def batch_query(self, questions: List[str]) -> List[Dict]:
        """ë°°ì¹˜ ì¿¼ë¦¬ (ë¹„ìš© íš¨ìœ¨ì )"""
        results = []
        total_cost = 0
        
        for q in questions:
            result = self.query(q)
            results.append(result)
            total_cost += result["cost"]["cost_usd"]
        
        print(f"ì´ ë¹„ìš©: ${total_cost:.4f}")
        return results
    
    def initialize(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.setup_embeddings()
        chunks = self.load_and_chunk_documents()
        self.setup_vectorstore(chunks)
        self.setup_llm()
        self.setup_retrieval_chain()

# ì‚¬ìš©
if __name__ == "__main__":
    # ì´ˆê¸°í™”
    rag = ProductionRAGSystem(
        data_path="./documents",
        index_name="company-knowledge-base",
        model="gpt-4-turbo-preview"
    )
    rag.initialize()
    
    # ë‹¨ì¼ ì¿¼ë¦¬
    response = rag.query("ìš°ë¦¬ íšŒì‚¬ì˜ ë³´ì•ˆ ì •ì±…ì€?")
    print("ë‹µë³€:", response["answer"])
    print("ë¹„ìš©:", response["cost"])
    print("ì¶œì²˜:", len(response["sources"]), "ê°œ ë¬¸ì„œ")
    
    # ë°°ì¹˜ ì¿¼ë¦¬
    questions = [
        "ì—°ì°¨ ì •ì±…ì€?",
        "ì¬íƒê·¼ë¬´ ê·œì •ì€?",
        "ë³µì§€ í˜œíƒì€?"
    ]
    results = rag.batch_query(questions)
```

## 4.2 í‰ê°€ ìë™í™” ì‹œìŠ¤í…œ

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset
import pandas as pd

class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ ìë™ í‰ê°€"""
    
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        ]
    
    def create_test_dataset(
        self,
        questions: List[str],
        ground_truths: List[str]
    ) -> Dataset:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"""
        data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truths": []
        }
        
        for q, gt in zip(questions, ground_truths):
            result = self.rag_system.query(q)
            
            data["question"].append(q)
            data["answer"].append(result["answer"])
            data["contexts"].append([
                s["content"] for s in result["sources"]
            ])
            data["ground_truths"].append([gt])
        
        return Dataset.from_dict(data)
    
    def evaluate_system(
        self,
        test_dataset: Dataset
    ) -> pd.DataFrame:
        """ì‹œìŠ¤í…œ í‰ê°€ ì‹¤í–‰"""
        print("ğŸ” í‰ê°€ ì‹œì‘...")
        
        result = evaluate(
            test_dataset,
            metrics=self.metrics
        )
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ
        df = pd.DataFrame([result])
        
        # ì„ê³„ê°’ ê²€ì¦
        thresholds = {
            "faithfulness": 0.8,
            "answer_relevancy": 0.7,
            "context_precision": 0.7,
            "context_recall": 0.8
        }
        
        print("\nğŸ“Š í‰ê°€ ê²°ê³¼:")
        for metric, threshold in thresholds.items():
            score = result[metric]
            status = "âœ…" if score >= threshold else "âŒ"
            print(f"{status} {metric}: {score:.3f} (ì„ê³„ê°’: {threshold})")
        
        return df
    
    def compare_versions(
        self,
        version_a,
        version_b,
        test_questions: List[str]
    ):
        """ë‘ RAG ë²„ì „ ë¹„êµ"""
        results = {"A": [], "B": []}
        
        for q in test_questions:
            results["A"].append(version_a.query(q))
            results["B"].append(version_b.query(q))
        
        # í†µê³„ ê³„ì‚°
        metrics_a = self._compute_metrics(results["A"])
        metrics_b = self._compute_metrics(results["B"])
        
        print("\nğŸ“Š A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"ë²„ì „ A: {metrics_a}")
        print(f"ë²„ì „ B: {metrics_b}")
        
        winner = "A" if metrics_a["avg_score"] > metrics_b["avg_score"] else "B"
        print(f"\nğŸ† ìš°ìŠ¹: ë²„ì „ {winner}")

# ì‚¬ìš©
evaluator = RAGEvaluator(rag)

test_questions = [
    "ìš°ë¦¬ íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì€?",
    "ì¬íƒê·¼ë¬´ ì‹ ì²­ ë°©ë²•ì€?",
    "ë³µì§€ í¬ì¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?"
]

ground_truths = [
    "ì—°ê°„ 15ì¼ì˜ ìœ ê¸‰íœ´ê°€",
    "ì¸íŠ¸ë¼ë„·ì—ì„œ ì‹ ì²­ì„œ ì‘ì„±",
    "ë³µì§€ëª°ì—ì„œ ì‚¬ìš© ê°€ëŠ¥"
]

# í‰ê°€ ì‹¤í–‰
dataset = evaluator.create_test_dataset(test_questions, ground_truths)
results = evaluator.evaluate_system(dataset)
```

---

# 5. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ì™€ ìµœì‹  íŠ¸ë Œë“œ {#section5}

## 5.1 2025ë…„ RAG ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 5.1.1 ë¬¸ì„œ ì²˜ë¦¬

**1) ìŠ¤ë§ˆíŠ¸ ì²­í‚¹**
```python
# âŒ ë‚˜ìœ ì˜ˆ: ê³ ì • í¬ê¸°
chunks = text.split_every(500)

# âœ… ì¢‹ì€ ì˜ˆ: ì˜ë¯¸ë¡ ì  ê²½ê³„
from langchain.text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " "],
    add_start_index=True
)
```

**2) ë©”íƒ€ë°ì´í„° ê°•í™”**
```python
for chunk in chunks:
    chunk.metadata.update({
        "source": doc.metadata["source"],
        "page": doc.metadata.get("page", 0),
        "section": extract_section(chunk),
        "date": doc.metadata.get("created_date"),
        "char_count": len(chunk.page_content),
        "word_count": len(chunk.page_content.split())
    })
```

### 5.1.2 ê²€ìƒ‰ ì „ëµ

**1) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**
```python
# ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰
from langchain.retrievers import EnsembleRetriever

ensemble = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]
)
```

**2) Reranking**
```python
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers import ContextualCompressionRetriever

# ì´ˆê¸° ê²€ìƒ‰ â†’ Reranking
reranker = CrossEncoderReranker(
    model_name="cross-encoder/ms-marco-MiniLM-L-12-v2"
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=base_retriever
)
```

### 5.1.3 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

```python
template = """ë‹¹ì‹ ì€ ì •í™•í•œ ì •ë³´ ì œê³µì„ ìš°ì„ í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì§€ì¹¨:**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
3. ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. ë‹µë³€ í›„ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

**ì»¨í…ìŠ¤íŠ¸:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€ (ì¶œì²˜ í¬í•¨):**
"""
```

### 5.1.4 ì—ëŸ¬ í•¸ë“¤ë§

```python
class RobustRAGSystem:
    def query(self, question: str) -> Dict:
        try:
            result = self.chain({"question": question})
            return {"success": True, "result": result}
        
        except VectorStoreError as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ì—ëŸ¬: {e}")
            return {
                "success": False,
                "error": "ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¥ì• . ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            }
        
        except LLMError as e:
            logger.error(f"LLM ì—ëŸ¬: {e}")
            return {
                "success": False,
                "error": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            }
        
        except Exception as e:
            logger.critical(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
            return {
                "success": False,
                "error": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
```

## 5.2 2025ë…„ ìµœì‹  íŠ¸ë Œë“œ

### 5.2.1 Long-Context RAG

**ê¸´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™œìš© (25,000+ í† í°)**

```python
# Claude 3 Opus: 200k í† í°
# GPT-4 Turbo: 128k í† í°
# Gemini 1.5 Pro: 1M í† í°

# ì „ì²´ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
llm = ChatAnthropic(
    model="claude-3-opus",
    max_tokens=4096,
    context_window=200000
)

# ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë³‘í•©
all_docs = "\n\n".join([doc.page_content for doc in docs])
response = llm.invoke(f"ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ: {all_docs}\n\nì§ˆë¬¸: {question}")
```

### 5.2.2 Adaptive RAG

**ìƒí™©ì— ë”°ë¼ ì „ëµ ë³€ê²½**

```python
def adaptive_retrieval(query: str):
    """ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ì „ëµ ì„ íƒ"""
    
    complexity = analyze_query_complexity(query)
    
    if complexity == "simple":
        # ê°„ë‹¨: ê¸°ë³¸ ê²€ìƒ‰
        return vector_search(query, k=3)
    
    elif complexity == "medium":
        # ì¤‘ê°„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        return hybrid_search(query, k=5)
    
    elif complexity == "complex":
        # ë³µì¡: ë‹¤ë‹¨ê³„ ê²€ìƒ‰ + reranking
        candidates = vector_search(query, k=20)
        return rerank(candidates, query)[:5]
```

### 5.2.3 Graph RAG

**ì§€ì‹ ê·¸ë˜í”„ì™€ RAG ê²°í•©**

```python
from langchain.graphs import Neo4jGraph

# ì§€ì‹ ê·¸ë˜í”„ ì¿¼ë¦¬
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# êµ¬ì¡°í™”ëœ ì •ë³´ + ë¹„êµ¬ì¡°í™”ëœ ì •ë³´
def graph_rag_query(question: str):
    # 1. ì—”í‹°í‹° ì¶”ì¶œ
    entities = extract_entities(question)
    
    # 2. ê·¸ë˜í”„ì—ì„œ ê´€ê³„ íƒìƒ‰
    graph_results = graph.query(f"""
        MATCH (e:Entity)-[r]->(related)
        WHERE e.name IN {entities}
        RETURN e, r, related
    """)
    
    # 3. ë²¡í„° ê²€ìƒ‰
    vector_results = vectorstore.similarity_search(question)
    
    # 4. ê²°í•©
    combined_context = merge(graph_results, vector_results)
    
    # 5. LLM ìƒì„±
    return llm.invoke(combined_context + question)
```

### 5.2.4 Multi-Modal RAG

**í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤**

```python
from langchain.llms import GPT4Vision

# ì´ë¯¸ì§€ í¬í•¨ RAG
image_docs = retrieve_images(query)
text_docs = retrieve_text(query)

# GPT-4 Visionìœ¼ë¡œ ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì²˜ë¦¬
response = GPT4Vision.invoke([
    {"type": "text", "text": f"ì§ˆë¬¸: {query}"},
    {"type": "text", "text": f"ì»¨í…ìŠ¤íŠ¸: {text_docs}"},
    {"type": "image_url", "image_url": image_docs[0]}
])
```

### 5.2.5 Federated RAG

**ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ì—°í•©**

```python
# íšŒì‚¬ ë‚´ë¶€ + ê³µê°œ ì›¹ + ì „ë¬¸ DB
def federated_search(query: str):
    results = {
        "internal": internal_vectorstore.search(query),
        "web": web_search_api(query),
        "papers": arxiv_search(query)
    }
    
    # ê° ì†ŒìŠ¤ì—ì„œ ìµœê³  ê²°ê³¼ ì„ íƒ
    return select_best_from_each(results)
```

## 5.3 í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 5.3.1 ì„±ëŠ¥

- âœ… ì‘ë‹µ ì‹œê°„ < 3ì´ˆ
- âœ… ë™ì‹œ ì‚¬ìš©ì 100+ ì§€ì›
- âœ… ìºì‹± êµ¬í˜„
- âœ… ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›

### 5.3.2 í’ˆì§ˆ

- âœ… Faithfulness > 0.8
- âœ… Answer Relevancy > 0.7
- âœ… ì¶œì²˜ ì¶”ì  ê°€ëŠ¥
- âœ… ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

### 5.3.3 ë¹„ìš©

- âœ… ì¿¼ë¦¬ë‹¹ ë¹„ìš© ì¶”ì 
- âœ… ì›”ë³„ ì˜ˆì‚° ì„¤ì •
- âœ… ì„ë² ë”© ì°¨ì› ìµœì í™”
- âœ… ëª¨ë¸ í¬ê¸° ì ì •í™”

### 5.3.4 ë³´ì•ˆ

- âœ… API í‚¤ ë³´í˜¸
- âœ… ì ‘ê·¼ ì œì–´ (RBAC)
- âœ… ë°ì´í„° ì•”í˜¸í™”
- âœ… ë¡œê·¸ ë§ˆìŠ¤í‚¹

### 5.3.5 ëª¨ë‹ˆí„°ë§

- âœ… ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
- âœ… ì—ëŸ¬ ì•Œë¦¼
- âœ… ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„
- âœ… A/B í…ŒìŠ¤íŠ¸

---

# ê²°ë¡ 

## í•µì‹¬ ìš”ì•½

**1) RAG ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ**
- ğŸ“„ Document Loaders
- âœ‚ï¸ Text Splitters
- ğŸ”¢ Embeddings
- ğŸ’¾ Vector Stores
- ğŸ” Retrievers
- ğŸ¤– LLM
- ğŸ”— Chains
- ğŸ’­ Memory (ì„ íƒ)

**2) í‰ê°€ ë°©ë²•**
- **ë…ë¦½ í‰ê°€**: ê²€ìƒ‰(Precision, Recall, MRR, nDCG), ìƒì„±(Faithfulness, Relevancy)
- **ì¢…ë‹¨ê°„ í‰ê°€**: Correctness, Latency, Cost, User Satisfaction
- **í”„ë ˆì„ì›Œí¬**: Ragas, DeepEval, LangSmith

**3) Agent ê°œë…**
- **ììœ¨ì„±**: ìŠ¤ìŠ¤ë¡œ ê²°ì •
- **ë„êµ¬ ì‚¬ìš©**: ì™¸ë¶€ ì‹œìŠ¤í…œ í˜¸ì¶œ
- **ì¶”ë¡ **: ReAct íŒ¨í„´
- **íŒ¨í„´**: Supervisor, Swarm, Planning

**4) 2025ë…„ íŠ¸ë Œë“œ**
- Long-Context RAG
- Adaptive RAG
- Graph RAG
- Multi-Modal RAG
- Federated RAG

## ë‹¤ìŒ ë‹¨ê³„

1. **í”„ë¡œí† íƒ€ì… êµ¬ì¶•**: ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì‹œì‘
2. **í‰ê°€ ì„¤ì •**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë©”íŠ¸ë¦­ ì •ì˜
3. **ë°˜ë³µ ê°œì„ **: í‰ê°€ ê²°ê³¼ ë°”íƒ•ìœ¼ë¡œ ìµœì í™”
4. **í”„ë¡œë•ì…˜ ë°°í¬**: ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ì¶œì‹œ
5. **ì§€ì†ì  ê°œì„ **: ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜

## ì¶”ê°€ ìë£Œ

- [LangChain ê³µì‹ ë¬¸ì„œ](https://python.langchain.com)
- [Ragas ë¬¸ì„œ](https://docs.ragas.io)
- [LangSmith](https://smith.langchain.com)
- [LangGraph ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)

---

**ì‘ì„± ì¼ì**: 2025ë…„ 11ì›” 9ì¼  
**ë²„ì „**: 1.0  
**ì—…ë°ì´íŠ¸**: ìµœì‹  LangChain ë° RAG ê¸°ìˆ  ë°˜ì˜
