---
layout: post
title: 딥러닝 모델 최적화와 배포
date: 2025-12-14
categories:
  - 배포
tags:
  - deeplearning
  - optimization
  - quantization
  - onnx
  - tensorrt
---
# 딥러닝 모델 최적화와 배포

## 목차
1. [딥러닝 모델을 ONNX, TensorRT 등의 포맷으로 변환해야 하는 이유](#section1)
2. [Post-Training Quantization과 Quantization-Aware Training의 차이](#section2)
3. [양자화/경량화 후 성능 검증을 위한 테스트 절차](#section3)

---

## 1. 딥러닝 모델을 ONNX, TensorRT 등의 포맷으로 변환해야 하는 이유 {#section1}

### ONNX (Open Neural Network Exchange)
**프레임워크 간 모델 호환성을 제공**하는 개방형 표준 포맷입니다.

주요 장점:
- **프레임워크 독립성**: PyTorch, TensorFlow 등에서 학습한 모델을 다른 환경에서 실행
- **배포 유연성**: 학습 프레임워크와 추론 엔진 분리 가능
- **경량 런타임**: 학습용 라이브러리 없이 추론만 수행
- **최적화 기회**: ONNX Runtime이 자동으로 그래프 최적화 수행

### TensorRT
**NVIDIA GPU를 위한 고성능 추론 엔진**으로, 최대 속도 최적화에 특화되어 있습니다.

주요 특징:
- **레이어 융합**: 여러 연산을 하나로 병합하여 메모리 접근 최소화
- **정밀도 조정**: FP32 → FP16 → INT8 자동 변환 및 캘리브레이션
- **커널 자동 선택**: GPU 아키텍처에 최적화된 연산 커널 선택
- **동적 텐서 메모리**: 메모리 재사용으로 메모리 사용량 감소

### 변환이 필요한 이유

**1. 추론 성능 향상**
- 원본 프레임워크 대비 2~10배 빠른 추론 속도
- 배치 처리, 레이어 최적화로 처리량 증가

**2. 리소스 효율성**
- 메모리 사용량 50~75% 감소
- 에지 디바이스 배포 가능

**3. 프로덕션 안정성**
- 학습용 무거운 의존성 제거
- 단순화된 배포 파이프라인

### 성능 비교 예시
```
PyTorch (FP32)     → 100ms (기준)
ONNX Runtime       → 45ms  (2.2배 향상)
TensorRT (FP16)    → 25ms  (4배 향상)
TensorRT (INT8)    → 15ms  (6.7배 향상)
```

---

## 2. Post-Training Quantization과 Quantization-Aware Training의 차이 {#section2}

### Post-Training Quantization (PTQ)
**학습 완료 후 모델을 양자화**하는 방식으로, 추가 학습 없이 적용 가능합니다.

동작 방식:
- 대표 데이터셋으로 활성화 값 분포 측정 (캘리브레이션)
- 가중치와 활성화 값의 범위를 분석하여 스케일 인자 계산
- FP32 → INT8/INT16 변환 적용

장점:
- 빠른 적용: 수 분~수 시간 내 완료
- 추가 학습 불필요
- 구현이 단순함

단점:
- 정확도 손실 가능성: 1~5% 정도 하락
- 복잡한 모델에서는 성능 저하 클 수 있음
- 미세 조정 불가능

적용 사례:
- 이미 성능이 충분히 높은 모델
- 빠른 프로토타이핑
- 정확도 손실이 허용 가능한 경우

### Quantization-Aware Training (QAT)
**학습 과정에서 양자화를 시뮬레이션**하여 모델이 양자화에 강건하도록 만듭니다.

동작 방식:
- Forward pass에서 양자화/역양자화 연산 삽입 (Fake Quantization)
- 양자화된 값으로 추론하지만 FP32로 역전파
- 모델이 양자화 노이즈에 적응하도록 학습
- Fine-tuning 또는 처음부터 학습 가능

장점:
- 최소 정확도 손실: 0.5% 이내로 유지 가능
- 극단적 양자화에도 안정적 (INT4, Binary)
- 양자화 인식 최적화 가능

단점:
- 학습 시간 증가: 기존 학습 시간의 1.5~2배
- 구현 복잡도 높음
- 추가 하이퍼파라미터 튜닝 필요

적용 사례:
- 엣지 디바이스용 초경량 모델
- 정확도가 매우 중요한 애플리케이션
- INT8 이하로 양자화해야 하는 경우

### 비교표

| 구분           | PTQ          | QAT            |
| ------------ | ------------ | -------------- |
| **학습 필요**    | 불필요          | 필요 (Fine-tuning) |
| **적용 시간**    | 수 분~수 시간      | 수 시간~수 일        |
| **정확도 손실**   | 1~5%         | 0.5% 이내        |
| **구현 난이도**   | 낮음           | 높음             |
| **적합한 비트**   | INT8         | INT4, Binary   |
| **사용 케이스**   | 빠른 배포, 프로토타입 | 프로덕션, 엣지 디바이스  |

### 선택 기준
```python
if 정확도_손실_허용 > 3% and 배포_시간_중요:
    선택 = "PTQ"
elif 극단적_경량화_필요 or 정확도_매우_중요:
    선택 = "QAT"
else:
    # PTQ 먼저 시도 후 성능 부족 시 QAT
    선택 = "PTQ → QAT"
```

---

## 3. 양자화/경량화 후 성능 검증을 위한 테스트 절차 {#section3}

### 1단계: 정확도 검증

**목표**: 양자화로 인한 성능 저하 정량화

필수 테스트:
- **벤치마크 데이터셋 평가**
  - 원본 모델 vs 양자화 모델 메트릭 비교
  - Accuracy, F1-Score, mAP 등 측정
  - 허용 범위: 일반적으로 1% 이내

- **Edge Case 분석**
  - 어두운 이미지, 노이즈 많은 오디오 등
  - 양자화로 취약해질 수 있는 샘플 집중 테스트

- **클래스별 성능 분석**
  - Confusion Matrix로 특정 클래스 성능 저하 확인
  - 불균형 데이터셋에서 소수 클래스 성능 모니터링


```python
# 정확도 검증 예시
original_acc = evaluate_model(original_model, test_loader)
quantized_acc = evaluate_model(quantized_model, test_loader)

accuracy_drop = original_acc - quantized_acc
assert accuracy_drop < 0.01, f"정확도 저하 {accuracy_drop:.2%} 초과"
```

### 2단계: 추론 성능 측정

**목표**: 실제 배포 환경에서의 성능 개선 확인

측정 항목:
- **Latency (지연 시간)**
  - 단일 샘플 추론 시간
  - P50, P95, P99 백분위수 측정
  - 콜드 스타트 vs 워밍업 후 성능

- **Throughput (처리량)**
  - 초당 처리 가능한 요청 수
  - 배치 크기별 성능 변화

- **메모리 사용량**
  - 모델 파일 크기
  - 런타임 메모리 피크
  - GPU VRAM 사용량


```python
# 성능 벤치마크 예시
import time
import numpy as np

latencies = []
for _ in range(1000):
    start = time.time()
    output = model(input_data)
    latencies.append(time.time() - start)

print(f"P50: {np.percentile(latencies, 50):.2f}ms")
print(f"P95: {np.percentile(latencies, 95):.2f}ms")
print(f"P99: {np.percentile(latencies, 99):.2f}ms")
```

### 3단계: 수치 안정성 검증

**목표**: 양자화로 인한 수치적 오류 탐지

검증 방법:
- **출력 분포 비교**
  - KL Divergence, Cosine Similarity 계산
  - 레이어별 출력값 분석

- **극단값 테스트**
  - 입력 범위의 최소/최대값 테스트
  - Overflow, Underflow 발생 여부 확인

- **재현성 검증**
  - 동일 입력에 대한 일관된 출력 확인
  - Non-deterministic 연산 모니터링


```python
# 출력 분포 비교 예시
from scipy.spatial.distance import cosine

original_output = original_model(test_input).flatten()
quantized_output = quantized_model(test_input).flatten()

similarity = 1 - cosine(original_output, quantized_output)
print(f"코사인 유사도: {similarity:.4f}")
assert similarity > 0.99, "출력 분포 차이 과다"
```

### 4단계: 실제 환경 시뮬레이션

**목표**: 프로덕션 환경과 유사한 조건에서 테스트

테스트 시나리오:
- **부하 테스트**
  - 동시 요청 수 증가시키며 성능 측정
  - 리소스 사용률 모니터링 (CPU, GPU, 메모리)

- **장시간 안정성 테스트**
  - 24시간 이상 연속 운영
  - 메모리 누수, 성능 저하 모니터링

- **실제 데이터 테스트**
  - 프로덕션 트래픽 샘플링하여 검증
  - A/B 테스트로 원본 모델과 비교


```python
# 부하 테스트 예시 (locust 사용)
from locust import HttpUser, task

class ModelLoadTest(HttpUser):
    @task
    def predict(self):
        self.client.post("/predict", 
                        json={"image": base64_image})
```

### 5단계: 모니터링 및 알람 설정

**목표**: 배포 후 지속적인 성능 추적

모니터링 메트릭:
- **실시간 정확도 추정**
  - 신뢰도 점수 분포 추적
  - 예측 불확실성 모니터링

- **성능 메트릭 대시보드**
  - Latency, Throughput 실시간 그래프
  - 에러율, 타임아웃 비율

- **알람 조건 설정**
  - 평균 지연 시간 > 임계값
  - 에러율 급증
  - 메모리 사용량 > 80%

## 정리

| 항목              | 핵심 내용                    | 주의사항                |
| --------------- | ------------------------ | ------------------- |
| **모델 변환**       | ONNX: 호환성, TensorRT: 성능 | GPU 아키텍처 호환성 확인     |
| **PTQ**         | 빠른 적용, 1~5% 정확도 손실      | 복잡한 모델은 성능 저하 클 수 있음 |
| **QAT**         | 최소 정확도 손실, 학습 시간 증가     | Fine-tuning 전략 중요   |
| **검증 절차**       | 정확도 → 성능 → 안정성 순차 검증    | 프로덕션 환경 시뮬레이션 필수    |
| **배포 전략**       | Canary → 점진적 확대          | 롤백 계획 사전 준비         |
