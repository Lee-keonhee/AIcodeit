








<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>AI 공부 블로그 | Deeplearning </title>
    <meta name="theme-color" content="#222222"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="/assets/js/jquery.min.js"></script>
    <script src="/assets/js/popper.min.js"></script>
    <script src="/assets/js/bootstrap4.min.js"></script>
    <script src="/assets/js/header.js"></script>
    <script src="/assets/css/svg-with-js/js/fontawesome-all.js"></script>
    <link href="/assets/css/bootstrap4.min.css" rel="stylesheet">
    <link href="/assets/css/theme.css" rel="stylesheet">
    <link href="/assets/css/syntax.css" rel="stylesheet">
 </head>

<body>






<script type="text/javascript">
    WebFontConfig = {
        google: {
            families: ['Ubuntu::latin']
        }
    };
    (function () {
        var wf = document.createElement('script');
        wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
            '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
        wf.type = 'text/javascript';
        wf.async = 'true';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(wf, s);
    })();
</script>

<nav class="navbar navbar-expand-lg fixed-top navbar-dark">
    <div class="container">
        <a class="navbar-brand" href="/">AI 공부 블로그</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse"
                data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup"
                aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
            <div class="navbar-nav">
                <a class="nav-item nav-link" href="/">home</a>
                <a class="nav-item nav-link" href="/archive">archive</a>
                <a class="nav-item nav-link" href="/tags">tags</a>
                <a class="nav-item nav-link" href="/about">about</a>
            </div>
        </div>
    </div>
</nav>

    <div class="wrapper">
      <div class="content">
        <div class="container container-center">
          <div class="row">
            <div class="col-md-8 offset-md-1">
              <div class="article">
                <div class="card">
                  <h1><a href="/2025/07/Deeplearning_basic/">Deeplearning</a></h1>
                  <div class="post-meta">
                    <div class="post-time">
                      <i class="fa fa-calendar-alt"></i>
                      <time>28 Jul 2025</time>
                    </div>
                    <ul>
                      
                        <li><a href="/tag/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88">딥러닝 기초</a></li>
                      
                    </ul>
                  </div>
                  <div class="post-content">
                    
                    <hr />
<h2 id="머신러닝-vs-딥러닝">머신러닝 vs 딥러닝</h2>
<hr />

<p>머신러닝은 데이터를 기반으로 학습하여 예측 또는 결정을 내리는 인공지능 분야입니다. 모델의 설계, 목표정의, 학습 데이터를 기반으로 하여, 최적의 Weight를 찾는 것이 머신러닝의 핵심입니다.<br />
반면, 딥러닝은 머신러닝의 하나의 방법론이지만 인공신경망, 즉 다중 퍼셉트론(MLP)에 기반을 둡니다. 딥러닝의 경우 머신러닝과는 다르게 사람의 개입없이 데이터의 특징을 추출하고 학습합니다. 뿐만 아니라, 여러 층의 신경망을 이용하여 복잡한 패턴을 처리합니다.<br />
이렇게 특징을 추출하는 능력을 가지고 있어, 머신러닝이 처리하기 힘든 비정형 데이터(이미지, 음성, 텍스트 등)과 같은 비정형 데이터 처리를 보다 쉽게 처리할 수 있습니다.</p>

<p>퍼셉트론</p>

<p>다층 퍼셉트론의 등장</p>
<ul>
  <li>하나의 퍼셉트론을 이용해서는 간단한 XOR문제도 해결할 수 없었다 이를 해결하기 위해 다층 퍼셉트론이 등장함</li>
</ul>

<p>다층 퍼셉트론의 다중분류</p>
<ul>
  <li>Softmax : i번째 클래스가 정답일 확률 pi  pi = e^z_i / sigma (e_)</li>
  <li>CrossEntropy</li>
</ul>

<p><br /></p>

<hr />
<h2 id="딥러닝의-성능-향상을-위해-고려해야-할-하이퍼-파라미터">딥러닝의 성능 향상을 위해 고려해야 할 하이퍼 파라미터</h2>
<hr />

<h2 id="1-활성화함수activation-function">1. 활성화함수(Activation Function)</h2>
<p>인공신경망에서 뉴런의 입력의 가중치 합을 받아 출력 신호로 변환하는 함수<br />
<br /></p>

<h3 id="활성화-함수의-역할">활성화 함수의 역할</h3>
<ul>
  <li><strong>비선형성</strong> 도입: 각 layer는 보통 선형함수로 되어있어, 비선형 함수 없이 계속 쌓는다고 하더라도 결국 선형 모델이 되어 비선형 관계를 학습할 수 없습니다. 이러한 한계점을 극복하기 위해 비선형성을 추가해주는 역할을 합니다.<br />
<br /></li>
</ul>

<h3 id="활성화-함수의-종류">활성화 함수의 종류</h3>

<ul>
  <li>
    <p>step function<br />
가중합이 0보다 작으면 0, 0보다 크면 1로 반환 - 미분이 안되어서 Sigmoid를 사용하게됨</p>
  </li>
  <li>
    <p>Sigmoid 함수</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Sigmoid</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>특징</td>
      <td>입력값을 0과 1 사이의 값으로 변환</td>
    </tr>
    <tr>
      <td>활용</td>
      <td>주로 이진 분류 문제의 출력층에서 0과 1 사이의 확률값 제공. 순환 신경망(RNN)에서 사용.</td>
    </tr>
    <tr>
      <td>한계</td>
      <td>기울기 소실(Vanishing Gradient) :입력값이 너무 크거나 작으면 기울기가 거의 0이 되어 역전파 과정에서 학습이 잘 이루어지지 않는 문제가 발생</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<ul>
  <li>하이퍼볼릭 탄젠트(Tanh) 함수</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Tanh</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>특징</td>
      <td>시그모이드 함수를 변형한 형태로, 입력값을 -1과 1 사이의 값으로 변환</td>
    </tr>
    <tr>
      <td>활용</td>
      <td>시그모이드와 마찬가지로 순환 신경망(RNN)에서 사용</td>
    </tr>
    <tr>
      <td>장점</td>
      <td>시그모이드 함수보다 출력값이 0을 중심으로 대칭적이기 때문에 학습이 더 효율적으로 진행</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<ul>
  <li>ReLU (Rectified Linear Unit) 함수</li>
</ul>

<table>
  <thead>
    <tr>
      <th>ReLU</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>특징</td>
      <td>입력값이 0보다 작으면 0을 출력, 0보다 크면 입력값을 그대로 출력(max(0, x))</td>
    </tr>
    <tr>
      <td>활용</td>
      <td>가장 널리 사용되는 활성화 함수 중 하나</td>
    </tr>
    <tr>
      <td>장점</td>
      <td>기울기 소실 문제를 완화, 계산이 단순하여 학습 속도가 빠름</td>
    </tr>
    <tr>
      <td>한계</td>
      <td>Dying ReLU(입력이 0 이하일 경우, 기울기가 0이 되어 한번 비활성화되면 활성화 되기 어려운 문제가 발생)</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<ul>
  <li>LeakyReLU</li>
</ul>

<table>
  <thead>
    <tr>
      <th>LeakyReLU</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>특징</td>
      <td>입력값이 0보다 작으면 아주 작은 양수 기울기가짐, 0보다 크면 입력값을 그대로 출력(max(0, x))</td>
    </tr>
    <tr>
      <td>활용</td>
      <td>ReLU의 한계점인 Dying ReLU문제를 해결</td>
    </tr>
    <tr>
      <td>장점</td>
      <td>음수 영역에서도 기울기가 존재하여 뉴런이 죽는 것을 방지, ReLU처럼 계산이 간단</td>
    </tr>
    <tr>
      <td>한계</td>
      <td>고정된 기울기가 항상 최적의 성능을 보장하지 못함</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<ul>
  <li>GELU</li>
</ul>

<table>
  <thead>
    <tr>
      <th>GELU</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>특징</td>
      <td>입력값에 표준정규분포의 누적 분포 함수(CDF)를 곱하는 형태, ReLU와 Dropout의 특성을 결합하여 부드러운(smooth) 비선형성을 제공</td>
    </tr>
    <tr>
      <td>활용</td>
      <td>BERT, GPT-3와 같은 NLP(자연어 처리) 분야의 트랜스포머(Transformer) 기반 모델에서 주로 사용. 컴퓨터 비전 분야의 Vision Transformer(ViT)나 MLP-Mixer에서도 활용됩니다 .</td>
    </tr>
    <tr>
      <td>장점</td>
      <td>ReLU보다 더 부드러운 비선형성을 제공, 잡음(noise)에 강인한 특성.</td>
    </tr>
    <tr>
      <td>한계</td>
      <td>ReLU나 LeakyReLU에 비해 계산 복잡도가 높음, 해석이 어려움</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="2-학습률learning-rate">2. 학습률(learning rate)</h2>
<p>학습률은 옵티마이저가 손실 함수의 최소값을 찾아가는 과정에서 각 반복(iteration)마다 가중치를 업데이트하는 스텝의 크기를 결정하는 매개변수입니다 .<br />
아래 그림과 같이, 학습률의 값이 너무 높을 경우 최적 지점을 지나쳐 발산할 위험이 있으며, 너무 낮을 경우 수렴 속도가 느려지거나 지역 최적점에 갇힐 수 있습니다.</p>

<p><img src="/assets/images/proper-step-size.png" alt="학습률" /><br />
(이미지출처-공돌이의 수학정리노트)</p>

<p>많은 학습에서 <strong>초기에는 큰 학습률</strong>로 빠르게 학습하고, <strong>후반부에는 작은 학습률</strong>로 미세 조정을 합니다.</p>

<h2 id="3-epoch-batch_size">3. epoch, batch_size</h2>
<ul>
  <li>
    <p>에포크(epoch)란?<br />
에포크는 전체 학습 데이터셋이 신경망 모델을 한 번 완전히 통과한 횟수를 의미합니다. 즉, 모든 훈련 데이터가 한 번씩 모델 학습에 사용되는 단위를 나타냅니다.</p>
  </li>
  <li>
    <p>배치 사이즈(batch size)란?<br />
배치 사이즈는 한 번의 가중치 업데이트를 위해 사용되는 훈련 샘플의 개수입니다. 전체 데이터셋을 여러 개의 작은 그룹으로 나누었을 때, 한 소그룹에 포함되는 데이터 수를 의미하기도 합니다.</p>
  </li>
  <li>
    <p>에포크와 배치 사이즈가 학습에 미치는 영향<br />
에포크 수가 너무 많으면 학습이 매우 오래 걸릴 뿐아니라, 과적합이 발생할 수 있습니다.<br />
배치 사이즈는 일반적으로 크기가 클수록 한 에포크당 학습 속도가 빠르지만, 메모리 사용량이 증가하게 됩니다.</p>
  </li>
</ul>

<h2 id="4-optimizer">4. optimizer</h2>
<p>옵티마이저는 모델의 손실 함수 값을 최소화하기 위해 가중치와 편향 등 모델의 파라미터들을 어떻게 업데이트할지 결정하는 알고리즘입니다. 경사 하강법 기반의 다양한 변형(예: Adam, SGD, RMSprop)이 존재하며, 최적화 과정의 효율성과 안정성에 직접적인 영향을 미칩니다.</p>

<h2 id="5-hidden-layer-수">5. hidden layer 수</h2>
<p>히든 레이어 수는 신경망의 입력층과 출력층 사이에 존재하는 은닉층의 개수를 의미합니다. 히든 레이어가 많아질수록 모델의 깊이가 깊어지며, 더 복잡하고 추상적인 데이터 특징을 학습할 수 있는 능력이 증가합니다. 그러나 과도한 레이어 수는 학습 비용을 증가시키고 과적합의 위험을 높일 수 있습니다.</p>

<h2 id="6-dropout">6. dropout</h2>
<p>드롭아웃은 신경망의 과적합(Overfitting)을 방지하기 위한 정규화(Regularization) 기법 중 하나입니다. 학습 과정에서 무작위로 선택된 뉴런들을 임시적으로 비활성화시켜 업데이트에 참여시키지 않음으로써, 각 뉴런이 다른 뉴런에 과도하게 의존하는 것을 방지하고 모델의 일반화 성능을 향상시키는 효과를 가져옵니다.</p>

<p><img src="/assets/images/dropout.png" alt="드롭아웃" /><br />
(이미지출처-a simple way to prevent neural networks from overfitting)</p>

                    

                  </div>
                  
                </div>
              </div>
            </div>
            <div class="col-md-3 hidden-xs">
              <div class="sidebar ">
  <h2>Recent Posts</h2>
  <ul>
    
    <li><a href="/2025/08/Image-pipeline/">이미지 파이프라인</a></li>
    
    <li><a href="/2025/08/Image-classification/">이미지 분류</a></li>
    
    <li><a href="/2025/08/Pytorch/">Pytorch</a></li>
    
    <li><a href="/2025/07/RNN/">RNN(Recurrent Neural Network)</a></li>
    
    <li><a href="/2025/07/LSTM/">RNN(Recurrent Neural Network)</a></li>
    
  </ul>
</div>

<div class="sidebar">
  <h2>Tags</h2>
  <ul class="fa-ul">
    
    
    <li><span class="fa-li"><i class="fas fa-tag"></i></span>
            <a href="/tag/Pandas" data-toggle="tooltip" data-placement="right" title="6">
        <span>Pandas</span></a></li>
    
    <li><span class="fa-li"><i class="fas fa-tag"></i></span>
            <a href="/tag/Python" data-toggle="tooltip" data-placement="right" title="6">
        <span>Python</span></a></li>
    
    <li><span class="fa-li"><i class="fas fa-tag"></i></span>
            <a href="/tag/code" data-toggle="tooltip" data-placement="right" title="4">
        <span>code</span></a></li>
    
    <li><span class="fa-li"><i class="fas fa-tag"></i></span>
            <a href="/tag/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88" data-toggle="tooltip" data-placement="right" title="10">
        <span>딥러닝 기초</span></a></li>
    
    <li><span class="fa-li"><i class="fas fa-tag"></i></span>
            <a href="/tag/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EA%B8%B0%EC%B4%88" data-toggle="tooltip" data-placement="right" title="7">
        <span>머신러닝기초</span></a></li>
    
  </ul>
</div>

            </div>
          </div>
        </div>
        

      </div>
          <footer class="footer-distributed">
      <div class="container">
        <div class="footer">
          <p>Pavel Makhov &copy; 2015-2018</p>
          <h6>Follow me</h6>

<ul class="social-media">

  
    <li>
      <a title="Lee-keonhee on Github" href="https://github.com/Lee-keonhee" target="_blank"><i class="fab fa-github fa-2x"></i></a>
    </li>
  

  
    <li>
      <a title=" on StackOverflow" href="https://stackoverflow.com/users/" target="_blank"><i class="fab fa-stack-overflow fa-2x"></i></a>
    </li>
  

  

  
    <li>
      <a title=" on Instagram" href="https://instagram.com/" target="_blank"><i class="fab fa-instagram fa-2x"></i></a>
    </li>
  

  
    <li>
      <a title=" on Last.fm" href="https://www.lastfm.com/user/" target="_blank"><i class="fab fa-lastfm fa-2x"></i></a>
    </li>
  

  
    <li>
      <a title="feed.xml RSS" href="/feed.xml" target="_blank"><i class="fas fa-rss fa-2x"></i></a>
    </li>
  

</ul>

        </div>
      </div>
    </footer>

    </div>
  </body>
</html>
