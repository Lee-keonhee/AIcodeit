<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>AI 공부 블로그</title>
		<description>Software Developer</description>
		<link></link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>이미지 파이프라인</title>
				<description>&lt;hr /&gt;
&lt;h2 id=&quot;1-이미지-파이프라인&quot;&gt;1. 이미지 파이프라인&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;이미지 파이프라인은 원본 이미지 파일을 모델 학습에 적합한 형태로 입력하기 위한 처리 단계를 의미&lt;br /&gt;
데이터 수집- 전처리(리사이즈, 정규화) - 증강 - Batch 화 등으로 이어지는 전체의 흐름&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-이미지-파이프라인의-핵심-구성요소&quot;&gt;2. 이미지 파이프라인의 핵심 구성요소&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 수집 및 저장:&lt;br /&gt;
파일 시스템, 아카이브(tar/webdataset), DB(LMDB/TFRecord) 등으로 원본을 관리합니다.&lt;/li&gt;
  &lt;li&gt;로드(I/O): &lt;br /&gt;
이미지 파일 읽기, 캐시, prefetch 등을 통해 디스크/네트워크 병목을 완화합니다.&lt;/li&gt;
  &lt;li&gt;전처리(Deterministic): &lt;br /&gt;
리사이즈, 센터크롭, 정규화 등 검증/추론용 일관된 처리.&lt;/li&gt;
  &lt;li&gt;증강(Augmentation): &lt;br /&gt;
학습용 무작위 변형(회전, 플립, 색상 변화 등) — on-the-fly 또는 오프라인 저장 방식 선택.&lt;/li&gt;
  &lt;li&gt;배치화 및 전송: &lt;br /&gt;
DataLoader 설정, num_workers, prefetch, pin_memory 등으로 GPU로의 전달을 최적화합니다.&lt;/li&gt;
  &lt;li&gt;캐싱/아카이빙: &lt;br /&gt;
변환 비용이 크면 오프라인으로 변환해 저장하거나 메모리/디스크 캐시 사용.&lt;/li&gt;
  &lt;li&gt;모니터링·로깅: &lt;br /&gt;
입력 품질 검사, 샘플 시각화, 메타데이터(어떤 증강을 적용했는지) 기록.&lt;/li&gt;
  &lt;li&gt;배포 파이프라인(추론용): &lt;br /&gt;
모델 서빙 앞단에서의 전처리·후처리와 모델 인퍼런스 흐름(클라우드 파이프라인으로 구성 가능)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;3-이미지-파이프라인-최적화-라이브러리&quot;&gt;3. 이미지 파이프라인 최적화 라이브러리&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;torchvision&lt;br /&gt;
transforms, DataLoader 등 파이프라인의 기준점 제공&lt;/li&gt;
  &lt;li&gt;Albumentations&lt;br /&gt;
빠르고 유연한 증강 라이브러리, 다양한 변환을 효율적으로 구성&lt;/li&gt;
  &lt;li&gt;Kornia&lt;br /&gt;
Pytorch 텐서 상에서 GPU로 증강을 수행할 수 있음, CPU 병목 해소&lt;/li&gt;
  &lt;li&gt;Nvidia Dali&lt;br /&gt;
데이터 로딩/전처리/증강을 GPU에서 처리해 대규모 학습에서 CPU 병목을 해소&lt;/li&gt;
  &lt;li&gt;pyvips / libvips / sharp (Node용) &lt;br /&gt;
대용량 이미지를 고속으로 리사이즈·변형할 때 탁월한 성능을 보이는 라이브러리(특히 오프라인 전처리에서 유리). sharp는 libvips 기반의 고성능 도구&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;단계별-라이브러리-선택&quot;&gt;단계별 라이브러리 선택&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;개발 및 디버깅 &lt;br /&gt;
torchvision + Albumentations // konia&lt;/li&gt;
  &lt;li&gt;본 훈련&lt;br /&gt;
DALI // konia + DataLoader 튜닝&lt;/li&gt;
  &lt;li&gt;오프라인 대량 전처리(리사이즈/캐시)&lt;br /&gt;
pyvips // sharp&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 14 Aug 2025 09:00:00 +0900</pubDate>
				<link>/2025/08/Image-pipeline/</link>
				<guid isPermaLink="true">/2025/08/Image-pipeline/</guid>
			</item>
		
			<item>
				<title>이미지 분류</title>
				<description>&lt;hr /&gt;
&lt;h2 id=&quot;1-이미지-분류&quot;&gt;1. 이미지 분류&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;이미지-분류-정의&quot;&gt;이미지 분류 정의&lt;/h3&gt;

&lt;p&gt;이미지 분류는 이미지 내의 특정 사물을 분류하는 것이다. 예를 들면, 아래 그림과 같이 강아지의 이미지를 보고 해당 사물을 강아지로 분류하는 것을 이미지 분류이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dog.jpg&quot; width=&quot;300&quot; height=&quot;200&quot; alt=&quot;강아지&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;mlpmulti-layer-perceptron의-한계&quot;&gt;MLP(Multi Layer Perceptron)의 한계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;이미지 평탄화(Flatten)작업이 필요 &amp;gt; 각 픽셀 간의 위치 정보를 잊게됨 &amp;gt; 인접 픽셀간 관계 학습이 어려움&lt;/li&gt;
  &lt;li&gt;평탄화된 모든 픽셀을 활용하여 pooutput 도출 &amp;gt; 파라미터 수 매우 증가 &amp;gt; 과적합 및 비용 증가&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnnconvolutional-neural-network의-등장&quot;&gt;CNN(Convolutional Neural Network)의 등장&lt;/h3&gt;

&lt;p&gt;이러한 다층 퍼셉트론의 한계를 보완하기 위해서 CNN이 등장하였다. CNN은 Convolution연산과 Polling을 통해, 이미지에서 특징(Feature)를 추출하는 네트워크이다.&lt;/p&gt;

</description>
				<pubDate>Thu, 14 Aug 2025 09:00:00 +0900</pubDate>
				<link>/2025/08/Image-classification/</link>
				<guid isPermaLink="true">/2025/08/Image-classification/</guid>
			</item>
		
			<item>
				<title>Pytorch</title>
				<description>&lt;hr /&gt;
&lt;h2 id=&quot;1-pytorch와-tensorflow&quot;&gt;1. Pytorch와 TensorFlow&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;Pytorch와 TensorFlow 모두 딥러닝 분야에서 가장 대중적으로 사용되는 프레임 워크입니다.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;TensorFlow&lt;/strong&gt;의 경우, 구글리서치의 구글브레인 팀이 공개한 라이브러리로써, Pytorch보다 먼저 상용화 되어 산업 분야에서 매우 큰 생태계를 형성하고 있습니다. TensorFlow의 경우, 초기에는 정적 계산 그래프 방식을 통하여 배포 시의 성능 최적화를 중점으로 두었습니다. 특히 대규모 환경에서의 안정적인 운영과 다양한 플랫폼으로의 배포를 위한 풍부한 도구들을 제공합니다.&lt;br /&gt;
하지만 이러한 정적 계산 그래프방식은 디버깅의 어려움과 유연성 부족이라는 단점을 가지고 있어, 2.0 버전부터는 Eager Execution을 도입하여, Pytorch와 마찬가지로 동적 계산 그래프 방식을 지원하게 되었습니다.&lt;/p&gt;

&lt;p&gt;※ Eager Excution : 연산을 즉시 실행하고 결과값을 알려주는 방식&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;의 경우, Facebook(현 Meta)의 AI연구팀에서 개발한 머신러닝 라이브러리로써, Define-by-Run방식을 통해 동적 계산 그래프 방식을 채택하고 있습니다. 이 방식은 코드를 실행하는 동시에 그래프를 구성하기 때문에 개발자가 모델의 흐름을 직관적으로 이해하고 디버깅하기 매우 용이합니다. 이러한 유연성 덕분에 연구 개발 단계에서 새로운 아이디어를 빠르게 실험하고 프로토타이핑하는 데 매우 효과적입니다.&lt;br /&gt;
pytorch는 현재 연구 및 개발 분야에서 빠르게 성장하고 있으며 이를 바탕으로, 과거에는 배포와 관련된 도구가 다양해지고 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-pytorch의-텐서란&quot;&gt;2. Pytorch의 텐서란&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;Pytorch에서 Tensor(텐서)란 딥러닝의 모델의 입력, 출력, 모델의 가중치를 표현하는 데 사용되는 자료구조입니다. &lt;br /&gt;
즉, 이러한 입,출력 및 가중치를 표현하기 위해 다차원적인 숫자 데이터의 형태를 띠는데, 이를 표현하고 연산하기 위해 ‘텐서’라는 다차원 배열 자료구조를 사용하는 것입니다.&lt;br /&gt;
쉽게 말해, 텐서는 배열이나 행렬과 비슷한 자료구조입니다.&lt;/p&gt;

&lt;h3 id=&quot;tensor와-numpy-array비교&quot;&gt;tensor와 numpy array비교&lt;/h3&gt;

&lt;p&gt;Tensor의 경우, ndarray와 같이 다차원을 배열을 다루기 위해 사용되며, 다양한 연산을 사용할 수 있다. 하지만 ndarray에서 지원하지 않는 GPU 가속을 통해, 병렬 연산에 최적화되어있으며, 역전파 계산을 위한 자동미분기능(autograd)을 내장하고 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;분류&lt;/th&gt;
      &lt;th&gt;특징&lt;/th&gt;
      &lt;th&gt;PyTorch 텐서&lt;/th&gt;
      &lt;th&gt;NumPy ndarray&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;공통점&lt;/td&gt;
      &lt;td&gt;자료구조&lt;/td&gt;
      &lt;td&gt;다차원 배열&lt;/td&gt;
      &lt;td&gt;다차원 배열&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;연산 기능&lt;/td&gt;
      &lt;td&gt;다양한 수치 연산 (사칙연산, 행렬 곱셈 등) 및 데이터 조작 기능 지원&lt;/td&gt;
      &lt;td&gt;다양한 수치 연산 (사칙연산, 행렬 곱셈 등) 및 데이터 조작 기능 지원&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;주요 데이터 표현&lt;/td&gt;
      &lt;td&gt;딥러닝 모델의 입력, 출력, 가중치 등 숫자 데이터를 표현&lt;/td&gt;
      &lt;td&gt;일반적인 수치 데이터 (과학 데이터, 이미지, 신호 등)를 표현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;차이점&lt;/td&gt;
      &lt;td&gt;GPU 지원&lt;/td&gt;
      &lt;td&gt;GPU 가속을 기본으로 지원하여 대규모 병렬 연산에 최적화&lt;/td&gt;
      &lt;td&gt;CPU에서 주로 작동하며, GPU 가속을 자체적으로 지원하지 않음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;자동 미분&lt;/td&gt;
      &lt;td&gt;자동 미분 기능(autograd)을 내장하여 역전파 계산에 필수적임&lt;/td&gt;
      &lt;td&gt;자동 미분 기능을 제공하지 않음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;목적 및 활용&lt;/td&gt;
      &lt;td&gt;주로 딥러닝 모델의 개발, 학습, 추론 등 딥러닝 생태계에 특화&lt;/td&gt;
      &lt;td&gt;범용적인 과학 계산, 데이터 분석, 그리고 딥러닝에서 데이터 전처리 등 다양한 분야에 활용&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;데이터 공유&lt;/td&gt;
      &lt;td&gt;NumPy 배열과 변환 시 (예: .numpy(), .from_numpy()) 대부분 데이터를 공유합니다.&lt;/td&gt;
      &lt;td&gt;PyTorch 텐서와 변환 시 데이터를 공유합니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;3-pytorch-텐서의-주요-기능-및-메서드-정리&quot;&gt;3. PyTorch 텐서의 주요 기능 및 메서드 정리&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;PyTorch 텐서는 데이터를 효율적으로 저장하고, 조작하며, 딥러닝 연산에 최적화된 다양한 메서드와 함수를 제공합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;텐서 생성 및 초기화&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.tensor(data)&lt;/code&gt;: Python 리스트, NumPy 배열 등의 데이터를 텐서로 만듭니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.zeros(shape)&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.ones(shape)&lt;/code&gt;: 모든 값이 0 또는 1인 텐서를 만듭니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.rand(shape)&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.randn(shape)&lt;/code&gt;: 균일 분포 또는 표준 정규 분포의 난수로 채워진 텐서를 만듭니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.empty(shape)&lt;/code&gt;: 초기화되지 않은 텐서를 생성합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;텐서 정보 확인&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.shape&lt;/code&gt; 또는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.size()&lt;/code&gt;: 텐서의 크기(각 차원의 길이)를 알려줍니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.dtype&lt;/code&gt;: 텐서의 데이터 타입(예: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.float32&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.int64&lt;/code&gt;)을 알려줍니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.device&lt;/code&gt;: 텐서가 현재 저장된 장치(CPU 또는 GPU)를 알려줍니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;텐서 연산&lt;br /&gt;
기본 산술 연산: +, -, *, / (요소별 연산). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.add()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.mul()&lt;/code&gt; 등 함수 형태로도 쓸 수 있습니다.&lt;br /&gt;
행렬 곱셈: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.matmul(tensor1, tensor2)&lt;/code&gt; 또는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor1 @ tensor2&lt;/code&gt;.&lt;br /&gt;
요소별 수학 함수: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.sqrt()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.exp()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.log()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.abs()&lt;/code&gt; 등 다양한 수학 함수를 텐서에 적용합니다.&lt;br /&gt;
집계(Aggregation) 연산: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.sum()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.mean()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.max()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.min()&lt;/code&gt; 등을 사용하여 특정 차원(dim)을 기준으로 값들을 합치거나 평균을 낼 수 있습니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;텐서 형태 변경 (Shape/View Manipulation)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.view(new_shape)&lt;/code&gt;: 텐서의 모양을 변경하지만 데이터는 복사하지 않습니다. (데이터가 메모리상 연속적일 때)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.reshape(new_shape)&lt;/code&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view&lt;/code&gt;와 유사하게 모양을 변경하며, 필요하면 데이터를 복사하여 연속성을 유지합니다. (더 유연)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.transpose(dim0, dim1)&lt;/code&gt;: 두 차원의 위치를 바꿉니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.T&lt;/code&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.t()&lt;/code&gt;)는 2D 텐서의 전치입니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.squeeze()&lt;/code&gt;: 차원의 길이가 1인 차원을 제거합니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.unsqueeze(dim)&lt;/code&gt;: 특정 위치에 길이가 1인 새로운 차원을 추가합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;인덱싱 및 슬라이싱&lt;br /&gt;
NumPy처럼 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor[idx, ...]&lt;/code&gt; 형태로 특정 요소나 부분 배열에 접근할 수 있습니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cat([tensor1, tensor2], dim)&lt;/code&gt;: 여러 텐서를 지정된 차원을 기준으로 연결합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;NumPy 호환 및 장치 이동&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.numpy()&lt;/code&gt;: PyTorch 텐서를 NumPy ndarray로 변환합니다. (메모리를 공유합니다.)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.from_numpy(ndarray)&lt;/code&gt;: NumPy ndarray를 PyTorch 텐서로 변환합니다. (메모리를 공유합니다.)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.to(device)&lt;/code&gt;: 텐서를 CPU에서 GPU로(예: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.to(&apos;cuda&apos;)&lt;/code&gt;) 또는 GPU에서 CPU로(예: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.to(&apos;cpu&apos;)&lt;/code&gt;) 이동시킵니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;자동 미분(Autograd) 관련&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.requires_grad = True&lt;/code&gt;: 이 텐서에 대한 모든 연산을 추적하여, 나중에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.backward()&lt;/code&gt;를 호출할 때 기울기(gradient)를 계산할 수 있도록 합니다.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor.detach()&lt;/code&gt;: 현재 계산 그래프에서 텐서를 분리하여, 이후 연산이 기울기 추적에 영향을 주지 않도록 합니다.&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Fri, 08 Aug 2025 16:00:00 +0900</pubDate>
				<link>/2025/08/Pytorch/</link>
				<guid isPermaLink="true">/2025/08/Pytorch/</guid>
			</item>
		
			<item>
				<title>RNN(Recurrent Neural Network)</title>
				<description>&lt;h3 id=&quot;rnn의-종류&quot;&gt;RNN의 종류&lt;/h3&gt;

&lt;h3 id=&quot;vanilla-rnn&quot;&gt;Vanilla RNN&lt;/h3&gt;
&lt;h3 id=&quot;hidden-state&quot;&gt;Hidden state&lt;/h3&gt;
&lt;h3 id=&quot;parameter-sharing&quot;&gt;parameter sharing&lt;/h3&gt;
&lt;h3 id=&quot;rnn의-손실함수&quot;&gt;RNN의 손실함수&lt;/h3&gt;
&lt;h3 id=&quot;bptt&quot;&gt;BPTT&lt;/h3&gt;
&lt;h3 id=&quot;bptt에서의-체인룰&quot;&gt;BPTT에서의 체인룰&lt;/h3&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 16:00:00 +0900</pubDate>
				<link>/2025/07/RNN/</link>
				<guid isPermaLink="true">/2025/07/RNN/</guid>
			</item>
		
			<item>
				<title>RNN(Recurrent Neural Network)</title>
				<description>&lt;h3 id=&quot;rnn의문제점&quot;&gt;RNN의문제점&lt;/h3&gt;
&lt;h3 id=&quot;lstm&quot;&gt;LSTM&lt;/h3&gt;
&lt;h3 id=&quot;forget-gate&quot;&gt;forget gate&lt;/h3&gt;
&lt;h3 id=&quot;input-gate&quot;&gt;input gate&lt;/h3&gt;
&lt;h3 id=&quot;output-gate&quot;&gt;output gate&lt;/h3&gt;

&lt;h3 id=&quot;-affine-변환-묶기&quot;&gt;?? affine 변환 묶기&lt;/h3&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 16:00:00 +0900</pubDate>
				<link>/2025/07/LSTM/</link>
				<guid isPermaLink="true">/2025/07/LSTM/</guid>
			</item>
		
			<item>
				<title>오토인코더(AutoEncoder)</title>
				<description>&lt;h3 id=&quot;표현학습&quot;&gt;표현학습&lt;/h3&gt;
&lt;h3 id=&quot;오토인코더란&quot;&gt;오토인코더란?&lt;/h3&gt;
&lt;h3 id=&quot;인코더-디코더-구조&quot;&gt;인코더-디코더 구조&lt;/h3&gt;
&lt;h3 id=&quot;오토인코더의-구조&quot;&gt;오토인코더의 구조&lt;/h3&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 16:00:00 +0900</pubDate>
				<link>/2025/07/AutoEncoder/</link>
				<guid isPermaLink="true">/2025/07/AutoEncoder/</guid>
			</item>
		
			<item>
				<title>CNN(Convolutional Neural Network</title>
				<description>&lt;hr /&gt;

&lt;h2 id=&quot;컴퓨터는-이미지를-어떻게-볼까&quot;&gt;컴퓨터는 이미지를 어떻게 볼까?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;흑백이미지의 경우, 이미지의 각 pixel의 명암을 기준으로 점점 밝아지면 255, 어두워지면 0에 가깝게 인식하여 이미지를 인식한다.&lt;br /&gt;
컬러 이미지의 경우, 각 이미지가 R, G, B 3개의 채널로 이루어져있으며, 각 이미지의 색상의 강도를 0~255 값으로 인식한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;fcn의-한계는&quot;&gt;FCN의 한계는?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;이미지일 경우, FCN에 입력하기 위해서는 각 이미지를 Flatten(평탄화) 작업을 하여, 각 픽셀 간의 위치정보를 잊게된다. 따라서, 인접 픽셀간의 관계를 학습할 수 없어 패턴학습이 어려워진다.&lt;br /&gt;
뿐만 아니라, 모든 픽셀을 연결해 하나의 output을 도출해야하므로 파라미터의 수가 매우 늘어나게 되고 과적합 및 계산 비용증가의 문제점이 발생할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;cnn&quot;&gt;CNN&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;이러한 FCN의 문제점을 극복하기 위해, Kernel을 이용한 합성곱(Convolution) 연산을 통해 지역적인 특징을 추출할 수 있는 신경망으로써, 주로 이미지 인식 및 분류에 특화된 인공신경망의 한 종류&lt;/p&gt;

&lt;h4 id=&quot;cnn의-특징&quot;&gt;CNN의 특징&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;부분 연결:&lt;br /&gt;
모든 노드 연결방식을 대체, 전체 이미지를 한번에 분석하지않고, 근접픽셀간의 관계부터 점진적 학습, 이미지의 지역적 정보를 효과적으로 학습&lt;/li&gt;
  &lt;li&gt;가중치 공유&lt;br /&gt;
동일한 필터(커널)을 이미지 전체에 적용, 파라미터의 수를 크게 줄여 계산 효율성을 향상&lt;/li&gt;
  &lt;li&gt;계층적 특징 표현&lt;br /&gt;
입력에 가까운 hidden layer에서는 단순한 특징(에지, 선)을 학습&lt;br /&gt;
출력에 가까운 hidden layer에는 추상적이고 복잡한 특징을 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convolutional-filter&quot;&gt;Convolutional Filter&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;커널(Kernel)이라고도 부르며, 이미지의 크기보다 작은 행렬&lt;/li&gt;
  &lt;li&gt;해당 Filter의 크기만큼의 영역에 대해 계산 수행&lt;/li&gt;
  &lt;li&gt;이미지 패턴 및 특징을 인식&lt;/li&gt;
  &lt;li&gt;색상이 있는 이미지는 RGB 채널에 각각의 가중치를 가지는 필터를 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;feature-map&quot;&gt;Feature Map&lt;/h4&gt;
&lt;p&gt;Filter를 통한 합성곱 연산 후에 나오는 출력값들의 2차원 배열로써, 시각적 특징들을 저장한 배열&lt;br /&gt;
입력 데이터에서 한 가지 특정한 특징 정보를 추출하고 그 위치와 강도를 2차원 형태로 표현한 결과물&lt;br /&gt;
-&amp;gt; 즉, 여러가지 필터를 통해, 입력 이미지의 서로 다른 종류의 특징을 감지 &lt;br /&gt;
(ex. 저수준 : 선, 모서리, 색상 변화 등, 고수준: 완전한 객체(고양이, 자동차), 전체적인 장면 등)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CNN_kernel1.png&quot; alt=&quot;kernel 계산&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/CNN_kernel2.png&quot; alt=&quot;kernel 계산&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/CNN_kernel3.png&quot; alt=&quot;kernel 계산&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;커널과 입력값 영역 선택&lt;br /&gt;
입력 이미지에서 커널 크기만큼 영역 선택&lt;/li&gt;
  &lt;li&gt;곱셈 및 덧셈 연산&lt;br /&gt;
선택한 영역과 커널 원소별 곱셈 후 편향값과 함께 더함&lt;/li&gt;
  &lt;li&gt;특성 맵 생성&lt;br /&gt;
출력값을 특성 맵에 저장 및 커널을 입력 이미지 전체에 스트라이드만큼 이동하며 반복&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Convolution 연산은 그 자체만으론 선형 계산이기 때문에, Activation funciton을 통해 비선형성 추가 -&amp;gt; 복잡한 데이터 학습&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;padding-stride&quot;&gt;Padding, Stride&lt;/h2&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;패딩padding&quot;&gt;패딩(Padding)&lt;/h4&gt;

&lt;p&gt;이미지 외곽에 픽셀 값을 추가(주로 0,zero padding)하여, 출력이미지의 크기를 조절함.&lt;br /&gt;
목적&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;필터 연산 시 이미지 가장자리 정보 손실 방지&lt;/li&gt;
  &lt;li&gt;출력 크기를 조정하여 원하는 네트워크 구조 유지&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stride&quot;&gt;Stride&lt;/h4&gt;

&lt;p&gt;필터의 이동 간격, 필터가 한번 이동할때 움직이는 pixel의 간격.&lt;br /&gt;
스트라이드가 커지게 되면 출력 특징맵이 작아지게됨.&lt;br /&gt;
합성곱 연산의 복잡도 줄임.&lt;/p&gt;

&lt;p&gt;출력 feature맵 크기 계산&lt;br /&gt;
Output_Size = int((Input Size - Filter Size + 2 * Padding) / Stride) + 1&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;receptive-field&quot;&gt;Receptive Field&lt;/h2&gt;

&lt;hr /&gt;
&lt;p&gt;CNN에서 출력(결과) 이미지의 한 점(픽셀/뉴런)이 입력(원본) 이미지의 어느 영역에서 영향을 받아 만들어졌는지를 나타내는 범위&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;# (따라서, Conv2의 출력에서 한 픽셀은 원본 입력 이미지의 5x5 영역을 보게 됩니다.)&quot;&gt;//&lt;/a&gt;: # (각 층의 효과 계산: 각 층이 RF에 기여하는 ‘크기’와 ‘점프’ 값을 계산합니다.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;# (따라서, Conv2의 출력에서 한 픽셀은 원본 입력 이미지의 5x5 영역을 보게 됩니다.)&quot;&gt;//&lt;/a&gt;: # (r_L: 해당 층의 출력 뉴런이 입력 이미지에서 ‘보는’ 크기 (리셉티브 필드 크기))&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;# (따라서, Conv2의 출력에서 한 픽셀은 원본 입력 이미지의 5x5 영역을 보게 됩니다.)&quot;&gt;//&lt;/a&gt;: # (가장 마지막 층인 Conv2의 한 픽셀이 Conv2의 입력(Conv1의 출력)에서 “보는” 크기는 Conv2의 커널 크기인 3입니다.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;# (따라서, Conv2의 출력에서 한 픽셀은 원본 입력 이미지의 5x5 영역을 보게 됩니다.)&quot;&gt;//&lt;/a&gt;: # (이 k2만큼의 영역이 다시 원본 입력에 매핑될 때, Conv1의 스트라이드 s1만큼의 ‘점프’를 고려해야 합니다.)&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;pooling-layer&quot;&gt;Pooling Layer&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;대충 다운샘플링, 차원 축소 이런거하는 층 - 계산량 감소&lt;br /&gt;
Max Pooling, Average Pooling&lt;br /&gt;
풀링의 크기와 stride 크기 같게해서 많이 사용&lt;br /&gt;
풀링커널은 가중치없고, 해당 데이터의 max, mean 채용&lt;/p&gt;

&lt;p&gt;장점 - 계산량 감소, 과적합 방지, 위치 불변성 강화(작은 변형에 특징 유지함)&lt;br /&gt;
단점 - 정보 손실가능성, 위치정보 손실, 고정된 연산방식&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;cnn-모델&quot;&gt;CNN 모델&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;convolutional Block = Convolutional Layer(convolution oper + activaiton) + Pooling&lt;/p&gt;

&lt;p&gt;CNN 모델의 순서 : input - Convolutional Block * n - Flatten layer - FCL_classification - softmax - output&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;lenet-5&quot;&gt;LeNet-5&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;손글자 숫자(MNIST 데이터 셋) 인식 및 분류를 위해 CNN의 최초 실용적 구현한 모델.&lt;br /&gt;
합성곱 층(CNN Layer)와 풀링 층(Pooling Layer)라는 구성요소 등장&lt;/p&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 16:00:00 +0900</pubDate>
				<link>/2025/07/CNN/</link>
				<guid isPermaLink="true">/2025/07/CNN/</guid>
			</item>
		
			<item>
				<title>딥러닝 용어 정리</title>
				<description>&lt;hr /&gt;

&lt;h2 id=&quot;퍼셉트론&quot;&gt;퍼셉트론&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;퍼셉트론(Perceptron)&lt;/strong&gt;은 인공신경망의 가장 기본적인 형태로, 인간 뇌의 뉴런을 단순화하여 모델링한 것입니다.&lt;/p&gt;

&lt;p&gt;주요 요소:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;노드(Node): 정보를 처리하는 기본 단위로, 입력값을 받아 가중치(W)를 곱합니다.&lt;/li&gt;
  &lt;li&gt;연결(Connection): 노드들이 연결되어 있으며, 이 연결을 통해 데이터가 입력부터 출력까지 흐릅니다.&lt;/li&gt;
  &lt;li&gt;동작 원리: 입력값에 가중치를 곱하여 모두 더한 값(가중합)이 임계값을 넘으면 1을, 넘지 못하면 0을 출력하는 단순한 이진 분류를 수행합니다.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
단층 퍼셉트론: 입력층과 출력층으로만 이루어진 구조로, 주로 이진 분류나 단순 회귀 문제에 활용됩니다. 하지만 비선형적이고 복잡한 데이터 학습에는 한계가 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;다층-퍼셉트론-mlp&quot;&gt;다층 퍼셉트론 (MLP)&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;다층 퍼셉트론(Multi-Layer Perceptron, MLP)&lt;/strong&gt;은 단층 퍼셉트론의 한계를 극복하기 위해 등장했습니다. 여러 개의 퍼셉트론이 계층(Layer) 형태로 연결된 구조를 가집니다.&lt;/p&gt;

&lt;p&gt;구조:&lt;br /&gt;
입력층(Input Layer)&lt;br /&gt;
은닉층(Hidden Layer): 하나 이상 존재할 수 있습니다.&lt;br /&gt;
출력층(Output Layer)&lt;br /&gt;
주요 특징: 단일 퍼셉트론이 해결할 수 없었던 XOR 문제와 같은 비선형적인 문제들을 해결할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;은닉층과-순전파역전파&quot;&gt;은닉층과 순전파/역전파&lt;/h2&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;은닉층 (Hidden Layer)&lt;br /&gt;
위치 및 역할: 입력층과 출력층 사이에 존재하는 층으로, 신경망의 핵심적인 학습이 이루어지는 곳입니다.&lt;br /&gt;
특징: 입력 데이터를 단순 변환하는 것을 넘어, 가중합과 비선형 활성화 함수를 사용하여 데이터 속에 숨겨진 복잡한 비선형 관계나 추상적인 특징을 파악하고 모델링합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;순전파 (Forward Propagation)&lt;br /&gt;
과정: 신경망의 입력층에서 시작하여 은닉층을 거쳐 출력층까지 데이터를 순차적으로 전달하며 예측값을 얻는 과정입니다.&lt;br /&gt;
이전 층의 노드 값에 &lt;strong&gt;가중치를 곱하고 편향을 더하는 선형 변환(Z)&lt;/strong&gt;을 수행합니다.&lt;br /&gt;
이 Z 값에 &lt;strong&gt;활성화 함수를 적용(A)&lt;/strong&gt;하여 해당 층의 출력을 계산합니다.&lt;br /&gt;
이 과정을 각 층마다 반복하여 최종 출력층에서 모델의 예측값을 얻습니다.&lt;br /&gt;
목표: 주어진 입력에 대한 모델의 예측을 생성합니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;역전파 (Backpropagation)&lt;br /&gt;
과정: 순전파를 통해 얻은 예측값과 실제 정답 사이의 오차(손실)를 기반으로, 신경망의 가중치와 편향을 업데이트하여 모델의 성능을 개선하는 알고리즘입니다.&lt;br /&gt;
출력층에서 계산된 오차를 손실 함수로 정량화합니다.&lt;br /&gt;
이 오차를 줄이기 위해 &lt;strong&gt;연쇄 법칙(Chain Rule)&lt;/strong&gt;을 사용하여 네트워크의 역방향(출력층 → 은닉층 → 입력층)으로 &lt;strong&gt;각 가중치와 편향에 대한 미분값(기울기)&lt;/strong&gt;을 효율적으로 계산합니다.&lt;br /&gt;
계산된 기울기 값을 활용하여 가중치와 편향을 조정합니다.&lt;br /&gt;
목표: 모델의 오차를 최소화하도록 파라미터를 최적화합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;활성화-함수&quot;&gt;활성화 함수&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;출력층의 활성화 함수는 신경망의 마지막 층에 위치하여, 처리된 결과를 우리가 원하는 최종 출력 형태로 변환하는 역할을 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;회귀 문제: &lt;br /&gt;
연속적인 실수 값을 예측하므로, 주로 &lt;strong&gt;선형 함수(Linear Function)&lt;/strong&gt;를 사용합니다. (가중합 값을 그대로 사용)&lt;/li&gt;
  &lt;li&gt;이진 분류 문제: &lt;br /&gt;
두 가지 중 하나를 분류하므로, &lt;strong&gt;시그모이드 함수(Sigmoid Function)&lt;/strong&gt;를 사용하여 0과 1 사이의 확률 값을 출력합니다.&lt;/li&gt;
  &lt;li&gt;다중 분류 문제: &lt;br /&gt;
여러 클래스 중 하나를 분류하므로, &lt;strong&gt;소프트맥스 함수(Softmax Function)&lt;/strong&gt;를 사용하여 각 클래스에 속할 확률을 출력합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;손실-함수와-최적화&quot;&gt;손실 함수와 최적화&lt;/h2&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;손실 함수 (Loss Function)&lt;br /&gt;
역할: 모델의 예측값과 실제 정답 값 사이의 오차(차이)를 계산하는 함수입니다.&lt;br /&gt;
중요성: 손실 함수 값이 작을수록 모델의 예측이 정답에 더 가깝다는 의미이며, 이 값을 기준으로 모델을 학습시킵니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;경사 하강법 (Gradient Descent)&lt;br /&gt;
역할: 손실 함수 값을 점차 최소화하여 모델의 최적 가중치를 찾아가는 방법입니다. 손실 함수의 기울기가 가장 가파른 방향(오차가 증가하는 방향)의 반대 방향으로 조금씩 이동하며 파라미터를 업데이트합니다.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
경사 하강법의 종류:
    &lt;ul&gt;
      &lt;li&gt;배치 경사 하강법 (Batch Gradient Descent)&lt;br /&gt;
특징: 전체 데이터셋을 한 번에 사용하여 가중치를 업데이트합니다.&lt;br /&gt;
장점: 손실 함수가 안정적으로 감소하며, 최적점에 수렴할 가능성이 높습니다.&lt;br /&gt;
단점: 대규모 데이터셋에서는 계산량이 매우 많고 느립니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;확률적 경사 하강법 (Stochastic Gradient Descent, SGD)&lt;br /&gt;
특징: 데이터를 한 번에 한 개씩만 사용하여 가중치를 업데이트합니다.&lt;br /&gt;
장점: 연산 속도가 빠르고, 메모리 효율이 좋습니다. 지역 최적점에서 탈출하기 용이합니다.&lt;br /&gt;
단점: 가중치 업데이트 방향이 불안정하여 수렴 과정이 진동하고 느릴 수 있습니다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;미니배치 경사 하강법 (Mini-batch Gradient Descent)&lt;br /&gt;
특징: 전체 데이터를 &lt;strong&gt;작은 묶음(미니배치)&lt;/strong&gt;으로 나누어 기울기를 계산하고 가중치를 업데이트합니다.&lt;br /&gt;
장점: 배치 GD와 SGD의 장점을 결합하여 연산 효율과 안정성을 동시에 얻을 수 있습니다. 가장 일반적으로 사용됩니다.&lt;br /&gt;
단점: 미니배치 크기 선택에 따라 성능 차이가 발생할 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 14:00:00 +0900</pubDate>
				<link>/2025/07/Deeplearning/</link>
				<guid isPermaLink="true">/2025/07/Deeplearning/</guid>
			</item>
		
			<item>
				<title>Pandas 핵심 함수와 메서드 code</title>
				<description>&lt;hr /&gt;

&lt;h2 id=&quot;pandas-자주-사용하는-메서드&quot;&gt;Pandas 자주 사용하는 메서드&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;지난 시간에는 pd.read_csv()와 같이 Pandas의 데이터 구조화 함수들을 다루었다. 이제는 데이터프레임과 시리즈 객체에 직접 적용하는 다양한 핵심 메서드들을 살펴볼 차례입니다. 이 메서드들은 데이터의 효율적인 조작, 변형 및 분석을 위한 필수적인 도구입니다. 본 글을 통해 실제 데이터 처리 과정에서 유용한 Pandas 메서드들을 소개한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;-4-결측치-중복치-처리&quot;&gt;   4. 결측치, 중복치 처리&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.isnull()&lt;/code&gt; / &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.isna()&lt;/code&gt;: &lt;br /&gt;
데이터프레임의 결측치를 True로 반환한다. ‘.sum()’를 활용하여 결측치 개수 확인할 수 있다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.fillna(value)&lt;/code&gt;: &lt;br /&gt;
결측치를 특정 value로 채운다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.dropna(axis)&lt;/code&gt;: &lt;br /&gt;
결측치가 있는 행(axis=0) 또는 열(axis=1)을 제거한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.duplicated(subset=None, keep=&apos;first&apos;)&lt;/code&gt;: &lt;br /&gt;
중복된 행을 True/False로 표시한다. subset으로 특정 컬럼만 지정하여 중복을 검사할 수 있으며, keep으로 ‘first’ (첫 번째만), ‘last’ (마지막만), ‘False’ (모두)를 표시할 수 있다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.drop_duplicates(subset=None, keep=&apos;first&apos;, inplace=False)&lt;/code&gt;:&lt;br /&gt;
중복된 행을 삭제한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;오늘은 Pandas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataFrame&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Series&lt;/code&gt;의 가장 기본적인 정보들을 한눈에 파악할 수 있는 메서드들을 살펴보았습니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.head()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.info()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.shape&lt;/code&gt;처럼 단순해 보이는 기능들이지만, 데이터를 처음 마주했을 때 전체적인 그림을 그리는 데 필수적인 도구입니다. 이 외에도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.columns&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.index&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.dtypes&lt;/code&gt; 같은 속성들은 데이터의 구조를 이해하는 데 큰 도움이 될 것입니다.&lt;/p&gt;

&lt;p&gt;더 많은 Pandas 함수와 메서드를 탐구하고 싶으시면, 아래 링크를 통해 확인할 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;2025-07-21-Pandas_function_method_3.md&quot;&gt;** Pandas 핵심 메서드: 데이터 조작 및 변형 (Chapter 2)**&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 05:00:00 +0900</pubDate>
				<link>/2025/07/Pandas_function_method_code_4/</link>
				<guid isPermaLink="true">/2025/07/Pandas_function_method_code_4/</guid>
			</item>
		
			<item>
				<title>Pandas 핵심 함수와 메서드 code</title>
				<description>&lt;hr /&gt;

&lt;h2 id=&quot;pandas-자주-사용하는-메서드&quot;&gt;Pandas 자주 사용하는 메서드&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;지난 시간에는 pd.read_csv()와 같이 Pandas의 데이터 구조화 함수들을 다루었다. 이제는 데이터프레임과 시리즈 객체에 직접 적용하는 다양한 핵심 메서드들을 살펴볼 차례입니다. 이 메서드들은 데이터의 효율적인 조작, 변형 및 분석을 위한 필수적인 도구입니다. 본 글을 통해 실제 데이터 처리 과정에서 유용한 Pandas 메서드들을 소개한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;-3-데이터-조작-및-변형&quot;&gt;   3. 데이터 조작 및 변형&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.drop(labels, axis)&lt;/code&gt;: &lt;br /&gt;
특정 행(axis=0) 또는 열(axis=1)을 삭제한다. 원본을 변경하려면 inplace=True를 추가한다. 즉, 따로 변수에 저장하지 않아도 업데이트된다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.rename(columns={&apos;원래이름&apos;: &apos;새이름&apos;})&lt;/code&gt;: &lt;br /&gt;
컬럼 또는 인덱스 이름을 변경한다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.replace(to_replace, value)&lt;/code&gt;: &lt;br /&gt;
특정 값을 다른 값으로 대체한다. 단일 값, 리스트, 딕셔너리 등 다양한 형태로 사용 가능하다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.set_index(keys, inplace=False)&lt;/code&gt;: &lt;br /&gt;
특정 컬럼(들)을 새로운 인덱스로 설정한다. inplace=True로 원본 변경 가능하다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.reset_index(level=None, drop=False, inplace=False)&lt;/code&gt;: &lt;br /&gt;
현재 인덱스를 컬럼으로 변환하고, 새로운 기본(0부터 시작하는) 정수 인덱스를 생성한다. drop=True로 기존 인덱스를 제거할 수 있다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.sort_values(by=&apos;컬럼명&apos;, ascending=True)&lt;/code&gt;: &lt;br /&gt;
특정 컬럼의 값을 기준으로 데이터프레임을 정렬한다. ascending=False로 내림차순 정렬할 수 있다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.value_counts()&lt;/code&gt; (Series 메서드): &lt;br /&gt;
Series 내 고유값들의 빈도수를 계산하여 Series로 반환한다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.astype(dtype)&lt;/code&gt;:&lt;br /&gt;
데이터프레임 전체 또는 특정 Series의 데이터 타입을 변경한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.astype({&apos;컬럼1&apos;: int, &apos;컬럼2&apos;: float})&lt;/code&gt; 와 같이 적용 가능하다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.groupby(&apos;기준_컬럼&apos;).agg(집계함수)&lt;/code&gt;: &lt;br /&gt;
특정 컬럼을 기준으로 데이터를 그룹화한 후, 각 그룹에 집계 함수(sum, mean, count 등)를 적용한다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.mean()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.sum()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.min()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.max()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.std()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.median()&lt;/code&gt;: &lt;br /&gt;
숫자형 컬럼에 대한 평균, 합계, 최솟값, 최댓값, 표준편차, 중앙값 등을 계산한다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.pivot_table(values=None, index=None, columns=None, aggfunc=&apos;mean&apos;)&lt;/code&gt;: &lt;br /&gt;
데이터프레임을 특정 컬럼들을 기준으로 재구성하여 요약 테이블(피벗 테이블)을 생성한다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;series.map(arg)&lt;/code&gt;: &lt;br /&gt;
Series의 각 값에 함수나 딕셔너리를 매핑하여 새로운 Series를 생성한다. Series에만 적용 가능하다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df.apply(any_fn, axis=0)&lt;/code&gt;:&lt;br /&gt;
각 열(axis=0) 또는 행(axis=1)을 &lt;strong&gt;Series 형태로&lt;/strong&gt; 함수의 인자로 넘겨 함수를 적용한다. 그 결과로 새로운 DataFrame이나 Series를 반환한다.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Series.apply(any_fn)&lt;/code&gt;:&lt;br /&gt;
Series의 각 &lt;strong&gt;개별 요소(값)&lt;/strong&gt;에 함수를 적용한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;오늘은 Pandas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataFrame&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Series&lt;/code&gt;의 가장 기본적인 정보들을 한눈에 파악할 수 있는 메서드들을 살펴보았습니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.head()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.info()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.shape&lt;/code&gt;처럼 단순해 보이는 기능들이지만, 데이터를 처음 마주했을 때 전체적인 그림을 그리는 데 필수적인 도구입니다. 이 외에도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.columns&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.index&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.dtypes&lt;/code&gt; 같은 속성들은 데이터의 구조를 이해하는 데 큰 도움이 될 것입니다.&lt;/p&gt;

&lt;p&gt;더 많은 Pandas 함수와 메서드를 탐구하고 싶으시면, 아래 링크를 통해 확인할 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;2025-07-21-Pandas_function_method_2.md&quot;&gt;** Pandas 핵심 메서드: 데이터 선택 및 필터링 (Chapter 2)**&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;2025-07-21-Pandas_function_method_4.md&quot;&gt;** Pandas 핵심 메서드: 결측치, 중복치 처리 (Chapter 2)**&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 29 Jul 2025 05:00:00 +0900</pubDate>
				<link>/2025/07/Pandas_function_method_code_3/</link>
				<guid isPermaLink="true">/2025/07/Pandas_function_method_code_3/</guid>
			</item>
		
	</channel>
</rss>
